# 游戏客户端八股文

## 自我介绍
面试官您好，我叫吴其炜，在去年6月份毕业于上海海洋大学，取得了软件工程的学士学位，目前是美国加州大学河滨分校的研究生一年级学生。

在本科阶段，我学习了基本的开发技能，包括设计模式等实用的开发技巧，并且我参加了一些算法竞赛并获得了奖项，这些经历锻炼了我的编程能力和逻辑思维。2022年暑假，我在深圳前海微众银行进行了为期几个月的实习，期间主要负责项目界面的制作，并参与了一个车展厅项目，其中我完成了智能语音客服的部分。这段经历让我对虚幻引擎的使用有了更深的理解。

现在，在研究生阶段，我进一步学习了计算机体系结构和操作系统这些课程，认识和了解更为现代的CPU结构和操作系统结构，同时，我加入了Zhijia Zhao教授的实验室，主要从事并行计算和数据解析方面的研究工作。我相信这些技术在游戏开发中也有广泛的应用。

作为一个多年的游戏玩家，希望我不仅仅只能从作为玩家的角度支持电子游戏，更是希望自己能切身的参与到电子游戏的开发中，为游戏行业添砖加瓦。谢谢！

## C++相关

### 多态与虚函数
1. 什么是多态
    多态是面相对象编程的一个核心概念，指的是对象(通过其引用和指针)可以表现出除它们自身类型以外的形式。多态的目的主要是允许一个接口表示不同的底层数据类型。**这样程序可以在运行时选择正确的方法而不是在编译时决定。**

2. 静态多态和动态多态
    * 静态多态(编译时多态): 主要通过函数重载和运算符重载实现。静态多态会在编译期间确定使用哪个一函数或者运算符。
    * 动态多态(运行时多态): 通过虚函数和继承方法实现，在运行时决定使用哪一个函数，依赖与对象的实际类型。

3. C++动态多态的实现
    C++中多态的实现主要通过虚函数，其中设计到两个非常重要的概念：虚函数表、虚函数指针
    * 虚函数表：
        定义：虚函数表是用于存放成员函数指针的数组，每个含有虚函数的类都有一个对应的虚函数表
        作用：当类的对象调用虚函数时，C++ runtime 会通过虚函数表来查找到需要调用的函数。这允许在运行时进行方法的绑定
        位置：虚函数表通常只存在与程序的**只读数据段(如全局数据段)**，而不是每一个对象的实例中。每一个类只有有一个虚函数表，多有对象共用一个表
        大小：虚函数表所占用的空间大小与虚函数的数量有关，等于`虚函数数量 * 函数指针大小(取决于操作系统)`
        创建时机：编译时，
        值得注意的是，如果一个派生类没有全部override基类的虚函数，则派生类中的虚函数表没有被override的函数指针是指向基类的虚函数。

    * 虚函数指针：
        定义：虚函数指针是指向虚函数表的一个指针
        作用：每一个含有虚函数的类的对象都会包含一个这样的指针，用于指向响应的虚函数表
        位置：虚函数指针通常存储在对象的内存布局的开始位置(取决于不同编译器的实现)。
        大小：与普通的指针一致，取决于操作系统，如果操作系统是32位的，那么这个虚函数指针就是4字节，64位的操作系统下就是8字节
        创建时机：对象构造时

4. 构造函数可以是虚函数吗？
    C++中构造函数不能是虚函数，主要的原因有3点：
    * C++对象实例化的机制：由于虚函数的机制是在对象的**生命周期内部**进行工作的，而构造函数是用于创建对象实例的。因此在我们调用构造函数的时候，虚函数指针可能还没有被初始化完毕。
    * 构造函数的工作方式与虚函数不符合：当我们调用派生类的构造函数时，构造函数的执行顺序通常是先执行基类的构造函数，再执行派生类的，如果构造函数是虚函数，那么可能在调用基类构造函数的过程中直接调用了派生类的构造函数而造成基类某些成员变量没有完成初始化操作。从而导致程序错误

5. 析构函数可以是虚函数吗？
    C++的析构函数可以使虚函数，并且在类中存在动态资源和被继承的情况下，理应把析构函数设置成虚函数，否则可能会导致通过基类指针进行对象析构的过程中无法正常释放存在于派生类中的动态资源。当然，这个问题可以通过将所有的动态资源都是用智能指针进行管理来解决，此时，即使我们的析构函数不是虚函数，所有的资源也都能被正确的释放。

6. 纯虚函数的作用
    作为接口，让子类进行实现，否则该子类不能被实例化直到某一个子类将所有的纯虚函数进行定义。

7. 运行时一个对象怎么在表中去找自己想调用的那个函数
    

### 模板偏特化
模板偏特化允许开发者给模板类或者模板函数提供特殊的实现，比如在程序中存在一个接收两个模板参数的类，分别是T1和T2，通过模板偏特化，我们可以编写一个特定情况是T1为int类型，而T2保持泛型的一个模板类，通过这个特点，程序员能灵活的实现针对不同情况的一种优化。

### 智能指针
C++的智能指针主要是提供了自动的动态内存资源管理，使程序员免于手动释放内存的困扰。
在C++中智能指针主要有3种：
1. `std::unique_ptr`
    std::unique_ptr提供了对动态分配的内存资源的唯一所有权，确保了同一时间内只有一个std::unique_ptr可以拥有该资源。
    * 这种唯一性是如何保证的呢？
        在unique_ptr中，其复制构造函数与赋值运算符都被禁用，从而保证了所管理对象的唯一性，但我们仍可以通过std::move来移交资源的所有权。
    * unique_ptr初始化和make_unique(C++14加入)的区别
        * 异常安全：make_shared提供了更好的异常安全机制，在构造对象的时候如果发生了异常，该函数会负责将已经分配好的动态资源回收，防止内存泄漏的产生
2. `std::shared_ptr`
    std::shared_ptr提供了多个指针共享一个对象的所有权，当最后一个指向这个资源的shared_ptr被销毁时释放。
    * 如何实现？
        std::shared_ptr通过维护一个引用计数器来实现共享所有全。每当一个新的shared_ptr指向同一个对象时，计数增加，反之，当shared_ptr销毁时，计数减少。
        * 当一个shared_ptr被销毁时是如何通知别的shared_ptr进行引用计数减少的呢？
            实际上引用计数不是被shared_ptr所拥有的，而是通过内部存在的一个控制快记录，这个控制块跟踪了有多少个shared_ptr实例在共享同一个资源，当一个shared_ptr被销毁的时候，其析构函数会被调用，这个析构函数负责将对应的控制快的引用次数减1。
            随后，在这个析构函数中，会检查引用计数是否已经降到0，若是为0，此时就会触发资源的回收。
    * 潜在问题
        * 循环引用问题：当使用具有父子关系的对象时，如果两个对象互相持有对方的shared_ptr，他们的引用计数就永远不会到0，导致内存泄露。
        * 性能开销问题：由于shared_ptr需要额外维护引用计数器，并且这个引用计数器是线程安全的，这意味着每一次的shared_ptr复制和销毁都需要进行原子操作，对于单线程而言可能影响不是很大，增加的操作只有引用计数的变化，但对于多线程程序来说，这意味着会导致整个程序的短暂停顿。
        * 线程安全问题：shared_ptr本身的操作是线程安全的，但是不能保证所指向的对象的操作一定是安全的。
    * shared_ptr初始化和make_shared(C++14加入)的区别
        * 内存分配角度：直接初始化通常需要两次内存分配，一次是shared_ptr本身，另一次是控制块的分配；而使用make_shared则是把这两次分配合并在了一次，使得性能更佳且减少了内存碎片
        * 异常安全：make_shared提供了更好的异常安全机制，在构造对象的时候如果发生了异常，该函数会负责将已经分配好的动态资源回收，防止内存泄漏的产生
    
3. `std::weak_ptr`
    std::weak_ptr被设计出来用于观察shared_ptr所管理的资源，主要目的就是解决shared_ptr的循环引用问题。weak_ptr不会影响其观察的对象的生命周期。
    * 工作原理
        * 在shared_ptr中我们提到了一个控制块，实际上这个控制块是weak_ptr和shared_ptr共享的，这个控制块是既包含了用于shared_ptr资源的引用计数以及用于weak_ptr的弱引用计数，**当控制块中的引用计数和弱引用计数都为0时，这个控制块才会被释放**。
        * 调用weak_ptr中的`expired()`函数或者尝试使用`lock()`函数获取对应的shared_ptr从而使用该weak_ptr所指向的对象的时候，前者会返回true或者是false来表示资源是否已经被释放，而后者则是会返回一个空的shared_ptr表示资源已经被释放。

### 构造函数和析构函数编写的注意事项
1. 构造函数
    * **尽可能使用初始化列表来初始化成员变量**，通常会比在构造函数体中进行赋值更加有效，因为使用初始化列表是直接初始化成员变量，而不是先构造出一个默认的成员变量然后在进行复制。
    * **异常安全问题**，如果构造函数中要抛出异常，需要保证对象的状态始终保持一致，也就是在创建动态资源后，需要抛出异常，我们需要在抛出异常前将前面创建好的动态资源进行释放，否则可能会导致内存泄露问题，或者考虑使用资源管理类，比如说智能指针来自动回收资源。
    * **避免在构造函数中调用虚函数**，在构造函数中调用虚函数可能并不会表现出多态行为，因为此时可能派生类并没有构造完成。
    * **不要在构造函数中使用该对象的地址**，因为此时对象可能还没有完全被构造完毕，如果以地址的形式将当前的对象传递出去，可能会导致其他函数使用了一个并没有完全被构造出来的对象，造成异常。
    * **避免在构造函数中使用锁**，如果在构造函数中使用了一个全局锁，有可能会导致死锁的现象发生，并且如果构造函数中上锁的过程发生了错误并且抛出了异常，可能会导致在没有释放已初始化的动态资源的情况下构造函数意外退出，最终导致内存泄漏。通常而言，在构造函数中使用锁就意味着在设计上存在问题，最好的方法是通过一个初始化函数进行一些需要使用到锁的初始化操作。

2. 析构函数
    * **不要在析构函数中抛出异常**，因为这可能会导致析构函数没有执行完成，也就是资源可能没有完全被释放，此时就会出现内存泄漏的问题，如果析构函数调用的函数可能会抛出异常，需要使用try catch块对异常进行捕获，保证不将异常传播出析构函数。
    * **避免在析构函数中调用虚函数**，同构造函数，析构函数不是在对象的生命周期中，而是生命周期的结束，因此在这里调用析构函数可能并不会表现出多态行为，而是会调用当前类对应的函数，因为这个类可能并不是派生类而是基类。

### this指针
在C++中，this指针是一个特殊的用于指向自己的指针，其特点有：
1. **隐式传递**：在每一个非静态成员函数中，this指针是作为参数隐式传递的，不需要在函数中显式进行传递。
2. **只能在成员函数中进行使用**：this指针只能在成员函数中进行使用，并不能在外部使用。
3. **this指针只能指向当前的对象，并不能被修改指向**：this指针是常量指针，不能被重新赋值，且永远指向调用的成员函数的对象的地址。

存储位置：
通常而言，this指针是通过保存在寄存器中传递给被调用的成员函数，但是如果寄存器不足以存入所有的参数的时候，有可能会被压入被调用成员函数的栈空间当中。

### C++编译过程
C++的编译主要有以下的四个步骤，分别是：预处理、编译、汇编、链接
1. 预处理
    在这一步主要处理以“#”号为开头的指令，包括对#define的宏的展开，#include的文件的插入，#if等指令的处理。
2. 编译
    在这一步中，会将上面处理好的cpp文件进行编译生成对应的汇编代码，在这个过程中包括了cpp的语法分析，语义分析以及进行各种优化。这些优化就包括去除冗余、循环展开等技术。
3. 汇编
    在这一步会将上一步生成的汇编代码转换为机器代码，生成目标文件，如`.o`或者`.obj`等。
4. 链接
    最后就是使用链接器将一个或多个目标文件与库文件链接起来，最终生成可执行文件。

### 静态库和动态库的区别
1. 静态库
    当一个程序使用一个静态库的时候，这个静态库会被打包进最终的可执行文件中，增加可执行文件的大小，但是相比于使用动态库，使用静态库最终生成的可执行文件可以独立运行而不需要别的库文件的依赖。
2. 动态库
    动态库则与静态库不同，其并不会被打包进最终的可执行文件中，所以在运行这个可执行文件的时候需要在特定的位置中存放有该动态库的文件。动态库的优点是易于更新，我们不需要重新打包整一个可执行文件，只需要不修改动态库中的API进行重新打包动态库文件即可。
    在内存使用上也不同，动态库文件实际上是具有可执行权限的，这意味着在使用动态库的程序执行的时候，实际上动态库文件是会独立运行的，然后将库文件的物理地址映射到可执行文件的虚拟地址当中。
    在存在有多个可执行文件都使用了同一个动态库的情况下，动态库实际上的运行实例只有一份，操作系统所做的只是把这一份实例的物理地址都映射到每一个可执行文件进程中的虚拟内存。因此，当动态库中需要保存数据的时候，实际上这个数据是保存在可执行文件的内存当中而不是动态库的内存当中，通过这种做法能使得整体的内存使用效率提高。

### C++函数调用的过程
C++的函数调用大致可以分为5个步骤
1. 参数准备
    当调用一个函数时，首先需要准备参数，通常来说这些参数对被**从右到左**存入栈中，或者是放入寄存器当中。这一步具体取决于编译器的实现。
2. 栈帧设置
    每一次函数进行调用的时候，都会在栈上创建一个栈帧。栈帧包含了函数的局部变量、用于存储寄存器的空间、返回地址等。通常而言在汇编中返回地址时使用%eax寄存器保存的，当然这取决于具体的计算机架构。
3. 函数体执行
    一旦栈帧设置完毕，程序计数器(PC)被设置为了函数体的起始位置时，开始执行函数中的代码。
4. 返回值计算
    执行完函数体后，通过跳转到栈帧中保存的返回地址来实现。
5. 栈帧清理
    函数执行完成后，需要清理为这个函数调用所创建的栈帧，栈指针(SP)和基指针(BP)会被恢复到函数调用前的状态
6. 控制权返回
    控制权返回到函数调用的下一条指令。

### C++ 内联(inline)函数
内联函数是C++的一种优化手段，在编译阶段，编译器可能会对标记为内联的函数进行展开(在被调用的地方进行展开)，当然，这种展开也不是说绝对的，编译器会根据实际的优化等级和具体的函数实现决定是否最终内联一个函数，也就是说一个函数即使被标记为内联，其最终编译出来的结果也可能不是内联，而一个没有被标记为内联的函数，却有可能被编译器优化为内敛函数。

通过使用内联函数，在某些情况下能提升性能，具体提升的方面可以从以下角度来看：
1. 减少参数的复制：对于很大的对象来说，通过将函数内联，可以直接省去了参数的复制过程，因此能在参数传递阶段减少开销从而优化性能。
2. 减少了调用函数创建栈帧和跳转等过程：通过内联函数，就不再需要执行函数调用的那一个过程了，也就是减少了创建和销毁栈帧的过程，因此性能就会得到优化
3. 提高缓存的命中率：由于将函数内联了，因此程序计数器在执行下一步操作的时候不需要跳转到其他的地方，而是可以直接使用下一条指令进行执行。跳转的操作有很大的概率会导致Cache miss的现象，因为被调用的函数的指令所存储的位置和调用者代码所存储的位置不一定是紧挨着的，因此会导致Cache miss的情况出现从而降低性能。

当然，内联函数也不全是优点，还有有如下的缺点的：
1. 由于在被调用的地方会展开，如果这个内联函数被多个调用者调用，如果都进行展开的话，会增大代码的体积。
2. 内联函数通常也有其适用场景，通常适用于较为小型的且被频繁调用的函数，而对于较少调用且复杂的大函数来说，将其内联可能不不会带来多大的性能提升，反而可能会因为代码体积膨胀而导致性能降低。

### C++STL容器
1. vector
    vector是STL标准模版库的一个容器，用于顺序、动态地存储元素。vector类的实现基于数组，在他的源码中，是通过保存一个指针实现的。
    vector的主要特征包括：动态大小、连续存储、随机访问、灵活性
    其中对于动态大小这个特点来说，当我们尝试给一个size==capacity的vector插入一个新元素时，vector类会自动的进行扩容。当我们插入元素不会触发vector扩容是，插入的时间复杂度为O(1)，当产生扩容的需求时，插入的时间复杂度为O(n)，由vector的扩容机制所决定，整体的插入时间复杂度为O(1)
    对于连续存储这个特点，这是因为vector的底层实现实际上就是使用一个指针指向一段连续的内存空间，所有保存在这个vector中的元素都会被存放在这个连续的空间中。因此，在顺序访问的情况下，vector的缓存性能很好。
    对于随机访问这个特点，同样是来自于vector对于数据存储的底层实现带来的特点，，因为使用的是一段连续的内存空间，因此我们可以通过使用index计算便宜量直接计算出我们需要访问的内存地址，这个计算的过程并不会收到vector长度的影响，因此他的时间复杂度是O(1)

    * vector的扩容过程
        1. 触发：当我们尝试给一个size==capacity的vector插入一个新元素时，vector类就会触发扩容
        2. 新容量计算：在进行扩容前，需要计算目标扩容的大小，这个大小通常是原vector的1.5到2倍，具体看不同的实现，这种增长策略目的是平衡内存使用和重新分配的次数，因为每次重新分配的时间复杂度为O(n)，通过指数级的增长我们可以减少重新分配的次数
        3. 分配新内存：紧接着是根据计算出来的新容量分配新的内存
        4. 元素迁移：完成内存分配后进行元素的迁移，将所有保存在原内存空间的数据全部重新迁移到新内存空间。在这个过程中，若是元素支持移动语意，则会调用其移动构造函数，否则通过拷贝构造函数。(这一步是造成扩容时间复杂度为O(n)的主要原因)
        5. 释放旧内存：一旦元素完成迁移，原来的内存将被释放
        6. 更新内部状态：完成内存释放后，会进行内部数据的更新，比如capacity的值，会更新为新分配的内存空间的大小

        从性能考虑上，我们在清楚某个vector将要存放多少数据的时候，尽可能使用reserve函数预分配好足够的空间，避免在不断的push_back的过程中进行扩容。

    * vector分配新内存是使用new还是malloc
        实际上在vector扩容的过程中使用的既不是new也不是malloc，而是使用标准分配器std::allocator实现的，在这个分配器的实现中，通常使用`::operator new`来分配内存，这个`::operator new`与我们平时使用的new不同，它所做的仅仅只是分配内存空间但不会初始化。
        那么为什么不直接使用malloc呢？这是因为在C++中，对象的生命周期非常重要，使用malloc可能会导致对象没有被正确的初始化。并且`::operator new`可以被重载，提供更灵活的内存策略

2. list
    list是STL标准模版酷的一个容器，实现了一个双向链表，允许程序员在序列的任一位置进行快速的插入和删除，但不同于vector，list并不支持随机访问，因为其实现是链表。
    list的主要特征包括：动态大小、双向遍历、内存分配上不同
    对于动态大小这个特点，std::list的大小可以根据需要动态的进行增加或减少，并且在增加过程中并不存在vector类中的扩容的现象。
    对于双向遍历这个特点，是由list容器的底层实现所提供的，双向链表会维护这个链表的头尾指针，我们可以通过头指针从前往后遍历，也能通过尾指针从后往前遍历。
    对于内存分配上的特点，同样是由list容器的底层实现是链表决定的，这决定了list容器中每一个元素之间不存在物理上连续的关系，仅仅只做到了逻辑上的连续。并且由于是链表，其缓存性能很差，基本上每一次访问下一个元素都会导致cache miss，不利于进行高性能的代码的编写。

    从性能方面考虑：
    list在任何位置的插入和删除操作的时间复杂度为O(1)，但寻找到需要进行插入和删除的位置的时间复杂度为O(n)
    在顺序访问方面，由于是链表，其缓存性能很差，基本上每一次访问下一个元素都会导致cache miss，不利于进行高性能的代码的编写。

3. deque
    TODO：
    std::deque是C++标准模板库中的一个序列容器，允许使用者在序列的前端和后端进行高效的元素插入和删除，同时提供随机访问。
    * deque的实现：
        deque在内存中由一组固定大小的数组块实现，这些数组块可以是在内存中不连续的，每一个std::deque都会维护一个中央控制结构，用于控制这些数据块(在实际的标准库实现中，块大小通常是根据元素类型的大小和系统的内存页面大小来选择的，以便高效地利用缓存和减少内存碎片)
    * deque的扩容：
        当deque预先分配的空间不足的时候进行元素的插入，就会触发扩容的过程，其中对于插入前端和后端具有不同的表现。
        * 当在后端添加元素且最后的数组块已经满了的时候，deque会分配一个新的数字块在当前块序列的末尾。
        * 当在前端添加元素且最前端的数据块已经满了的时候，deque会分配一个新的数字快在当前块序列的开头。
        在更新块序列后，需要更新中央控制结构，用于管理各个数据块的指针，对于需要扩容的情况，也就是需要在中央控制结构中添加新添加的数组块的指针。
    * deque的优点：
        **局部扩容**：通过局部扩容机制，使得在不影响其他数据块的情况下进行扩容，减少了vector中扩容时产生的移动构造或者说复制构造。
        **提供前后插入且支持随机访问**
        **均摊为常数的时间复杂度**
    * deque的缺点：
        **额外的内存开销**：由于需要使用一个中央控制结构来控制整一个deque，因此其内存开销会比vector更大，且由于每一个数据块在物理上不一定是连续的，因此存在潜在的内存碎片问题。
        **实现复杂**：由于deque的实现比起vector而言更为复杂，因此其性能比起vector而言会更差，但对于一个能支持随机访问且能前后修改的数据结构而言，比起链表已经好很多了。
    * 中央控制结构的内容：
        1. 数据块指针：包含了指向各个数组块的指针，这些指针是std::deque的底层存储单元，每一块都能存储多个信息。
        2. 容量信息：控制结构中还包含了当前已分配块数量和总容量的信息。

4. map
    std::map是C++中提供键值对存储的容器，存储的是唯一的键对应唯一的值的情况。
    map的底层是通过红黑树实现的，这种数据结构帮助map容器能保证在动态插入后能维持查询的高性能而不是像最简单的二叉搜索树一样可能会失去平衡。

    * 从性能方面考虑：
        map的插入、删除和查找操作，时间复杂度均为O(logn)
        同样的，树型数据结构的缺点就是缓存性能很差，因为每一个节点都是新分配的空间，我们不能保证其在物理上连续，因此如果是需要反复进行遍历的数据而言，使用vector要优于map。

    * 从内存效率方面考虑：
        由于map是树状结构，每一个节点需要保存子节点的指针，因此每一个节点都会比单纯的数据要更大

    * 与unordered_map的比较：
        由于底层实现是红黑树，因此对于需要保持键的有序性的情况在，使用map优于unordered_map，因为unordered_map的实现原理是hash，并不保证元素的存放是有序的。但是我们若是不要求有序，则unordered_map能提供更高的查询性能，通常为O(1)

    对于map的键是自定义类的时候，需要注意的是我们需要重载`operator<`符号保证std::map能对我们的自定义类键进行排序插入到红黑树中。

5. unordered_map
    std::unordered_map是C++中提供键值对存储的容器，存储的是唯一的键对应唯一的值的情况，与std::map类似，但底层实现不同
    unordered_map的底层实现是通过hash进行计算的，通过使用hash函数计算得到的index将元素保存到指定位置的桶中，**桶在unordered_map中的定义是存在hash冲突的键所存放的数据结构，通常由链表或者数组实现(C++的unordered_map是用链表来实现的)**。
    * 从性能上分析：
        理想情况在unordered_map的插入、删除和查找的时间复杂度为O(1)，但如果我们的hash函数选择的不好，无法很好的解决hash冲突的问题是，时间复杂度有可能会退化到O(n)

    * 从内存效率方面分析：
        相比起map而言，如果我们的hash函数选择的好，unordered_map可能可以更高效的使用内存，因为不需要保存如子节点指针等额外信息。

    * unordered_map的桶扩容过程
        当unordered_map中的size==bucket_size的时候(也可以说负载因子为1.0)，就会触发unordered_map的桶扩容，通过桶扩容达到缓解hash冲突的问题。
        1. 创建新的哈希表：扩容时，会创建一个新的哈希表，这个新的哈希表桶的数量通常是原来的两倍。有助于分散原来冲突的元素，减少碰撞
        2. 重新计算hash：对于原哈希表中的所有元素，重新根据现在的桶大小计算哈希，然后存入对应的位置。
        3. 释放旧的哈希表：完整所有元素的哈希重新计算与存入后，释放旧的哈希表。

        桶的扩容与vector类似，都是非常消耗性能的操作，时间复杂度是O(n)，并且在扩容过程中会短暂的提高内存占用。因此，在了解我们需要一次性插入的数据的数量时，提前调用`reserve`函数进行提前分配空间从而避免过多的扩容操作。

    * 与map的比较：
        与map相比，unordered_map不能保证元素的有序性，但因此提高了增删查的性能，具体需要通过使用场景判断使用哪一个容器更合适

    对于unordered_map的键是自定义类的时候，我们需要重载`operator==`符号来保证查询过程中能进行相等比较，同时我们需要重载`std::hash<T>`模板的特化提供自定义的hash函数，帮助插入、删除和查询。

6. set
    与map类似，但是存的值是单个元素而非键值对。
    set同样能保证数据的有序性，并且还包括一个去重的特点

7. unordered_set
    与unordered_map类似，但是存的值是单个元素而非键值对。
    unordered_set不能保证数据的有序性，但仍保留数据去重的特性

8. queue

9. stack

### C++类继承
1. public继承
    * 基类的公共成员在派生类中仍然是公共的。
    * 基类的受保护成员在派生类中仍然是受保护的。
    * 基类的私有成员在派生类中是不可访问的。

2. protected继承
    * 基类的公共成员和受保护成员在派生类中都变为受保护的。
    * 基类的私有成员在派生类中是不可访问的。

3. private继承(默认)
    * 基类的公共成员和受保护成员在派生类中都变为私有的。
    * 基类的私有成员在派生类中是不可访问的。

### C++多线程
#### C++多线程的同步机制
1. 使用互斥锁：在下方有介绍
2. 条件变量
3. 原子操作
4. 屏障(C++20)
5. 信号量(C++20)

#### C++中的锁
1. std::mutex
    提供最基本的互斥操作，用于简单的避免多个线程同时访问共享数据，通过`lock()`函数上锁，`unlock()`函数解锁

2. std::recursive_mutex
    与std::mutex类似，但允许同一个线程进行多次的加锁，且不会导致死锁。（非同一线程则会像遇到mutex一样的情况）

3. std::timed_mutex
    提供带有超时的互斥锁机制，允许尝试加锁一段时间，若是在规定时间内没有成果获取到锁，则返回false，否则返回true；
    使用`try_lock_for()`和`try_lock_until()`这两个函数进行尝试。

4. std::recursive_timed_mutex
    是recursive_mutex和timed_mutex的结合，既可以允许同一个线程进行多次的加锁，也可以让其他线程进行一段时间内的加锁尝试。

5. std::shared_mutex (C++17)
    提供了读写锁的机制，允许多个读线程同时访问共享数据，但在写线程获取锁后变为独占。
    通过`lock_shared()`进行读上锁（共享），`lock()`进行写上锁（独占）

6. std::shared_timed_mutex (C++14)
    结合了std::shared_mutex和std::timed_mutex这两者的特性

#### C++中锁的辅助类
1. std::lock(是一个函数而不是类)
    std::lock函数可以帮助将多个锁进行同时上锁，从而避免了死锁的产生。
    通常而言，这个std::lock函数在使用后需要手动对所有的锁进行解锁，因此会配合如std::unique_lock等RAII工具对锁进行管理，代码如下
    ```cpp
    std::mutex mutex1, mutex2;

    std::lock(mutex1, mutex2); // 锁定多个互斥量，避免死锁
    std::unique_lock<std::mutex> lock1(mutex1, std::adopt_lock);  // 是使用adopt接管一个已经上锁了的锁
    std::unique_lock<std::mutex> lock2(mutex2, std::adopt_lock);

    // TODO：执行需要互斥访问的操作
    ```

2. std::lock_guard
    std::lock_guard是一个用于使用RAII对锁进行管理的一个工具类，他会在构造函数阶段上锁，然后在析构函数中解锁，从而在一些函数中程序员可以不用在函数或者一个作用域的结尾进行显式的解锁，提高了多线程编程的安全性和便利性。

3. std::unique_lock
    std::unique_lock是一个比起std::lock_guard更为灵活的一个工具，它在lock_guard的基础上，支持了延迟加锁、转移锁的所有权等操作，同时他还支持对condition_variable的自动管理。

4. std::scoped_lock (C++17)
    功能类似于std::lock_guard，但是支持对于多个锁的同时上锁，从而避免了死锁，代码如下
    ```cpp
    std::mutex mutex1, mutex2;
    std::scoped_lock lock(mutex1, mutex2);  // 得益于C++17的自动类型推断，不需要显式的在<>里面指定类型
    // 执行需要互斥访问的操作
    ```

#### 无锁多线程怎么协作？
1. 使用原子操作，std::atomic
    使用原子操作，std::atomic，这个标准库中的工具就是用来实现原子操作的，它利用的是处理器架构中的原子指令来实现，因此并没有使用到锁，所以可以将临界区的大小控制到最小从而获得最佳的并行性能。
    但是，如果CPU架构并不支持原子操作，那么会退回到使用锁来进行实现。此时的性能就变差了。

2. 使用无锁数据结构
    通过使用无锁的数据结构，比如无锁队列、无锁栈等数据结构。

3. 条件变量
    条件变量是C++进行多线程同步的一种机制，必须要配合互斥锁来进行实现，因此条件变量并不是一个锁。当一个线程使用锁后完成了对临界区的访问后，就可以将锁进行解锁，然后使用条件变量通知一下个步骤的线程仅需往下进行，以此达到类似于流水线的模式。
    ```cpp
    std::mutex mtx;
    std::condition_variable cv;
    bool ready = false;

    void worker() {
        // 等待主线程发送准备信号
        std::unique_lock<std::mutex> lock(mtx);
        cv.wait(lock, [] { return ready; });

        // 执行一些工作...
    }

    void main_thread() {
        // 准备工作
        {
            std::lock_guard<std::mutex> lock(mtx);
            ready = true;
        }
        cv.notify_all();  // 通知所有等待的工作线程

        // 继续执行其他操作...
    }
    ```

4. 使用互斥锁：在上方有介绍

5. 屏障(C++20)
    通过使用屏障，允许一组线程互相等待，直到所有的线程都达到某一个同步点，然后一起往下执行。

6. 信号量(C++20)
    通过使用信号量能是一定数量的线程同时访问共享资源，通常用于控制资源的并发访问。

7. Read Copy Update
    RCU主要是用于读多写少的场景，被广泛利用在操作系统内核等系统级程序当中，在某种程度上能被视为读写锁的改进，优化了读写锁的开销。其工作原理如下：
    * 对于读取操作：读取线程可以直接读取到数据，不需要加锁
    * 对于更新操作：写入线程首先会将要更新的数据复制一份，然后更新这个复制出来的副本，最后在通过原子操作std::atomic来实现将旧数据更新为新数据的操作，最后在没有更多的读取线程读取旧副本的时候进行旧资源的回收

    对于RCU而言，其优势在于不用加锁，因此在读多写少的场景下非常高效；并且相比于其他的无锁编程技术，如无锁队列等数据结构，其的编写更为清晰易懂。
    但不是说完全没有劣势：第一个劣势就是需要额外的内存来存储创建的副本；旧数据的回收可能不是立刻进行的，而是在没有读线程访问旧副本的时候才会真正的回收。不过，相比起它的优点而言，其缺点几乎可以忽略不计。


### C++ Lambda表达式
语法：
```cpp
[capture] (agrs lists) -> return type {
    function body
};
```
从汇编上看，lambda表达式的使用和普通的函数调用基本一样，但其存储位置和普通的函数不同，主要取决于其是如何定义和使用的
1. 栈：如果Lambda表达式是直接被定义在函数的内部并且没有被捕获为函数指针或者没有转换为std::function的情况下，则通常会存放于栈中。
2. 堆：如果Lambda表达式被转换为std::function或者被动态分配，比如说使用new创建或者使用shared_ptr这种智能指针进行管理的话，这个Lambda表达式就会存储到堆中。
3. 静态存储：如果Lambda表达式被定义为静态变量或者全局变量，就会被存储在静态/全局存储区中。

Lambda表达式的内存模型：
Lambda表达式的内存布局通常是由编译器实现的，因此不同的编译器可能会存在不同的内存布局。
对于值捕获的Lambda函数而言，在内存中的实际布局可能是如下的：
```sql
+-------------------+
| int a             |  <-- 第一块内存存放 int 类型的 a
+-------------------+
| double b          |  <-- 第二块内存存放 double 类型的 b
+-------------------+
| std::string c     |  <-- 第三块内存存放 std::string 类型的 c
+-------------------+
| ...               |  <-- 其他可能的内部数据，如 vtable 指针（如果 lambda 有虚函数）
+-------------------+
```
而对于引用捕获的Lambda函数，则是将捕获进来的对象的指针保存进内存中的头部，可能的内存布局如下
```sql
+-----------------------+
| int* ptr_to_a         |  <-- 第一块内存存放指向 int 类型的 a 的指针
+-----------------------+
| double* ptr_to_b      |  <-- 第二块内存存放指向 double 类型的 b 的指针
+-----------------------+
| std::string* ptr_to_c |  <-- 第三块内存存放指向 std::string 类型的 c 的指针
+-----------------------+
| ...                   |  <-- 其他可能的内部数据，如 vtable 指针（如果 lambda 有虚函数）
+-----------------------+
```

如果让一个auto func指向一个lambda函数，对这个函数进行sizeof，结果是什么？
这个取决于该lambda函数的捕获列表，对于没有捕获的lambda函数而言，其大小只有1。

### 空类的大小
空类的大小通常是1个字节，这一个字节通常是用来保证一个对象的唯一性，从而保证一个空对象拥有一个唯一的地址。

### 内存对齐
内存对齐是一种内存访问优化的技术，确保了数据结构是以起始地址按照一定的边界，通常是(2,4,8等字节)进行对齐。
为什么要内存对齐？
1. 提高缓存的性能
    由于计算机从内存中取得缓存的过程是一块一块的，这个块的大小通常是64B，如果我们不进行内存对齐，一个简单的int类型数据只占4B，但可能横跨两个块，这就会导致无论是我们想要获取这个数据也好还是从缓存中将新的值写入回内存这个过程都需要操作两个块，也就是128B，而我们的修改仅仅只是4B。这导致了巨大的带宽压力同时减少缓存性能。
2. 硬件要求
    有时候一些硬件对于数据的存储是有要求的，比如某些处理器可能只能从4个字节或者8个字节对齐的地址进行数据的读取，如果数据没有对齐，则需要进行额外的操作从而导致性能降低。

### static和const关键字
1. static的用法
    1. static关键字在类成员中的用法
        * 可以通过static关键字声明静态成员变量，这个静态成员变量会在程序执行main函数前进行初始化，并且其不属于任何一个对象，而是属于这个类本身
        * 可以通过static关键字声明静态成员函数，静态成员函数可以让我们在没有类对象实例的情况下进行调用，需要注意的是，静态成员函数中不能使用非静态成员变量，但可以使用静态的成员变量。
    2. static关键字在函数作用域中的用法
        * 我们可以通过在函数作用域中升级一个静态的变量，这个变量会在第一次调用这个函数时初始化，并程序执行的剩余时间里一直保持存在，直到程序执行结束
    3. static关键字在文件作用域上的用法
        * static关键字作用于全局变量和普通的函数时，可以将其的可见性限制在当前的文件内，使得其他的文件无法使用extern等方法访问。

2. const关键字的用法
    * 当const关键字作用于变量的时候，可以用于声明一个常量，这个常量在第一次初始化后便不能再修改。
    * 当const关键字作用域指针的时候，其作用取决于const相对于星号的位置，当const位于星号的左侧时，即`const int *`，这时，我们指针指向的值不能被改变，但是我们能改变这个指针的指向；当const位于星号的右侧时，即`int* const`，这时，我们的指针不能被改变，但是指针指向的值我们可以修改。
    * 当const作用于类成员函数时，表示这个成员函数不会修改这个类的状态，比如如果想要在一个const成员函数中修改一个成员变量时，编译器会报错，告诉我们不能这么做。

    对于一个普通的const变量，如果我们在声明后没有进行初始化，将会产生编译错误，但如果这个变量是类中的成员变量，我们能通过构造函数的初始化列表进行初始化；另外，如果我们在声明const变量时在前面添加上一个extern关键字，就可以在声明的时候不进行初始化，但是必须要在另外的文件中进行初始化。

### const和constexpr的区别
1. const
    const关键字用于声明一个常量，表示该值在初始化后的程序运行期间不能再被改变。
    const的值可以在**运行时**进行确定，不一定要在编译时就知道他的值，比如说我可以把一个常量初始化成一个用户输入的值，这种情况下这个常量的值只有在运行时才能确定。但const常量同样能在**编译时**进行确定。
2. constexpr
    constexpr关键字用于声明一个**编译时常量**，表示这个值必须要在编译的时候就确定好。
    除了修饰变量以外，还能用来修饰函数和用户定义的类型（比如类和结构体），要求函数的返回值或者类型的构造函数必须是编译时常量。举个例子：
    ```cpp
    constexpr int factorial(int n) {
        return (n <= 1) ? 1 : (n * factorial(n - 1));
    }
    int main() {
        constexpr int result = factorial(5);  // 在编译时计算 5 的阶乘
    }
    ```
    在举一个constexpr类的例子
    ```cpp
    class Point {
    public:
        constexpr Point(double x, double y) : x_(x), y_(y) {}

        constexpr double getX() const { return x_; }
        constexpr double getY() const { return y_; }

    private:
        double x_;
        double y_;
    };

    int main() {
        constexpr Point p(1.0, 2.0);  // 在编译时创建 Point 对象
        constexpr double x = p.getX();  // 在编译时获取 x 坐标
        constexpr double y = p.getY();  // 在编译时获取 y 坐标
    }
    ```

### new/malloc的区别
1. 从语言层面上来说，malloc是C语言的一个函数，而new是C++语言中的操作符。
2. 从内存初始化层面上来说，malloc函数的作用仅仅只有分配内存空间而不会初始化，但new操作符不仅会分配空间，还会调用对象的构造函数进行初始化，如果类型为基本数据类型时，则会初始化为默认值。
3. 从返回类型上来讲，malloc函数返回的是`void*`类型的指针，而new操作符会根据数据类型自动初始化，不需要程序员手动进行指针类型的转换。
4. 从错误处理上来讲，malloc过程中若是发生错误，则会返回NULL，而new操作符会抛出std::bad_alloc异常，除非我们使用了nothrow版本的new操作符，此时若是发生错误，则会返回NULL，就像malloc一样
5. 从使用角度上来讲，malloc需要程序员使用sizeof运算符手动计算需要分配的内存空间大小，而new操作符会自动计算所需的空间大小。
6. 从内存对齐方面，new操作符通常保证了更好的内存对齐，在C++中编译器知道每种类型的内存对齐要求，因为我们通常是以new Type的形式进行内存分配。但是malloc函数的使用过程中我们并没有指定我们想要的类型，因此malloc函数并不清楚我们所想创建的类型的内存对齐信息。

### C++内存分布
主要来说C++的内存分布可以被分为5个部分，分别是：堆、栈、静态/全局存储区、常量存储区、代码区
1. 堆
    C++的堆主要用于分配内存，通常通过操作如`new`和`delete`或者C语言中的`malloc`和`free`进行操控。
    * 如何禁止一个类的对象在堆中分配
        要实现这个非常简单，我们只需要在这个类中将new和delete操作符设置为私有，或将其定义为delete，此时当我们就无法使用new和delete在堆中创建这个对象。代码如下：
        ```cpp
        class NoHeapAlloc {
        private:
            // 重载new和delete操作符并设置为私有，阻止在堆上分配
            void* operator new(size_t size) = delete;
            void operator delete(void* pointer) = delete;

        public:
            NoHeapAlloc() {}  // 构造函数
            ~NoHeapAlloc() {} // 析构函数
        };
        ```
    * 在堆中分配资源的不同策略
        * **首次适应**：在这个策略中，内存管理器会从堆的开始位置向后搜索，直到碰到第一个足够大的空闲块。这种策略的优点是能快速分配，但可能会导致内存碎片。
        * **最佳适应**：在这个策略中，内存管理器会搜索整一个堆，找到一个最接近请求大小的空闲块进行分配，这种方法可以减少内存碎片，只留下比较小的无法利用的内存碎片，但整个搜索会比较耗时。
        * **最差适应**：在这个策略中，内存管理器同样会搜索整一个堆，然后使用最大的空闲块进行分配，这种方法目的是减少大块内存的分割，但可能会留下很多比较小的空闲块，导致碎片化。
    
        在不进行allocator重写的情况下，堆的分配策略遵循操作系统的默认策略

2. 栈
    C++的栈主要用于存储直接声明的变量，比如`int a = 0;`此时这个a变量就存储在栈中。这些变量由程序自动创建和销毁，不需要程序员的介入。
    一般来说，栈的内存分配和回收都很快，因为他是线性和连续的，通常是用于存储函数参数和局部变量。
    * 如何禁止一个类的对象在栈上进行分配？
        我们可以通过将所有的构造函数和析构函数设置为私有，并且暴露一些静态函数用于不同情况下的通过new操作符进行创建类对象，以及相应的destroy函数用于销毁对象。代码如下：
        ```cpp
        class NoStackAlloc {
        private:
            NoStackAlloc() {}  // 私有构造函数
            ~NoStackAlloc() {} // 私有析构函数

        public:
            static NoStackAlloc* CreateInstance() {
                return new NoStackAlloc();
            }

            static void DestroyInstance(NoStackAlloc* instance) {
                delete instance;
            }
        };
        ```

3. 全局/静态存储区
    顾名思义，这部分就是用来存储全局变量和静态变量的区域，存储在这部分里的变量会在程序开始时进行空间的分配(Lazy static变量除外)，直到程序结束时才会销毁。
4. 常量存储区
    顾名思义，常亮存储区就是用来存储常量的，比如字符串字面量和用const关键字声明的变量。这部分内存通常是只读的。
5. 代码段
    这部分内存用于存储这个程序在边以后的代码，用来告诉计算机这个程序该怎么运行，这部分也是只读的。

### 菱形继承类的内存布局


### 指针和引用的区别
1. 指针
    指针本质上就是一个存储在栈中的变量(局部中声明的指针，若是在全局范围内声明的指针则是存储与全局/静态存储区中)，只不过它的用途是用来存储一段地址，这个地址指向的就是真正的数据。

2. 引用
    引用的本质就是一个变量的一个别名，他是已经存在的一个变量的另一个名字

3. 区别
    * 他们的空值不同：指针允许有空值，就是常用的nullptr，但是引用不能存在空值，他一定是某一个已经存在的变量的另一个名称。这也说明了通常使用引用是更加安全的，因为引用保证了永远有一个被引用的对象存在，但是指针不一定。
    * 内存占用上也不同：指针通常会占用4到8字节的内存空间，这与程序运行的操作系统有关，32位的操作系统下指针就是4个字节，64位就是8个。但是引用通常不会占用内存空间（这取决于不同编译器的实现，这是因为一些编译器会在编译的过程中将引用替换为了对对象的直接访问）。
    * 在重新赋值这方面上也不同：指针的指向可以被改变。但是引用一旦初始化为执行一个对象后，就不能再被改变了。

4. 野指针
    野指针指的是指针变量指向了一个不明确的内存地址，通常是指一个未被初始化的指针。

5. 悬空指针
    悬空指针是指指向的资源已经被回收了，但是指针本身的值没有修改为nullptr或者NULL，此时指针仍然保存着之前的值，叫做悬空指针。

### C++的四种cast
1. static_cast
    这是最基本最常用的类型转换方式，用于在相关类型之间转换，比如说int转变为float，派生类指针转换为基类指针。这种cast的类型检查发生在编译时期。
2. dynamic_cast
    这种转换是用来在基类和派生类之间进行转换的，主要就是用于将基类指针转换为派生类。这种cast的类型检查是发生在运行时期的，因此叫做dynamic。
    * 对于指针类型，转换失败会返回nullptr
    * 对于引用类型，转换失败会报出一个`std::bad_cast`异常
3. const_cast
    这种转换是用于将原本的声明为const的常量的const去掉从而使其能被修改。
4. reinterpret_cast
    这种转换是最不安全的转换，它的作用是将原本的数据重新解释为另一种类型的数据。并且这种转换在编译时期和运行时期都不会进行检查。

### 左值引用和右值引用
1. 左值引用
    左值引用是对一个对象的持久身份的引用。在C++中，左值引用通常表示的是可以在等号左边出现的值的引用。这种引用常用于函数中参数的传递，避免函数调用过程中出现不必要的大对象的复制操作，这复制过程通常非常耗时。左值引用的特点就是不能绑定在字面量（1，2等数字）和临时对象（一个函数的返回值）上。
2. 右值引用
    右值引用就是对一个对象临时身份的引用，他指的是可以在等号右边出现的值但是不能在等号的左边出现。这种引用常用语引用临时对象，如函数的返回值，减少不必要的赋值构造，从而优化性能。
    * 右值引用的底层实现

    * 如何将一个左值转换为右值呢？
        通过C++的std::move函数即可

### 完美转发
C++的完美转发指的是，在函数模版中，能够以参数原本的类型（左值或者右值）进行传递并且转发到另一个函数的能力，这个主要通过std::forward函数进行实现。其中涉及到了C++的引用折叠的规则。
* 引用折叠
    在C++中存在的引用折叠的规则，简单来说就是允许一个类型的右值引用折叠成左值或者保持右值的一种特性，举个例子来说，假如我有一个函数的参数是`int&&`，当我给这个函数传入的是一个左值引用的时候，此时这个类型就会变成`int&& &`，随后C++就会对这个类型的引用进行折叠，实际就是减去2个&符号，因为我们原来由三个&符号，减去两个后就只剩下一个，表示的就是左值引用，而如果我们是传入右值引用，那么4个&符号就会减去2个&符号，剩下两个，保持了右值引用的类型，此时我们就保证了参数的传入是保持了它本身的引用状态。

### 内存泄漏的解决和检测
* 解决内存泄漏：
    使用智能指针或者在设计类的时候遵循RAII原则，通过在构造函数或者初始化函数中获取动态资源和在西沟函数中释放动态资源，从而使得动态资源被对象的生命周期所管理，从而解决内存泄漏

* 检测内存泄漏：
    1. 常用的IDE基本都有检测内存泄漏的能力，如VS中的内存诊断工具。
    2. 通过代码审查，检查是否有存在使用了new之后没有进行delete的情况出现。
    3. 通过日志，在申请动态资源的时候进行日志记录，在释放的时候也进行一次日志记录，通过查看日志检查是否存在只有申请却没有释放的资源。

## 数据结构算法

### 红黑树和AVL树
1. AVL树是一种自平衡的二叉搜索树，每一个节点都维护着一个平衡因子，这个平衡因子只能是1，0，-1。当AVL树进行插入或者删除的操作的时候，如果平衡因子超出了[-1, 1]这个范围，则需要进行左旋、右旋或者双旋进行高度的调整。使其最终能保证平衡。
    其时间复杂度（查询、插入、删除）的时间复杂度都是O(logn)，但由于AVL树的平衡条件过于严苛，因此在插入和删除的过程中需要调整的节点会比较多，因此整体性能上没有红黑树等其他自平衡结构效率高。

2. 红黑树同样是一种自平衡的二叉搜索树，其等价于234B树，但是其自身的平衡条件并没有这么严格，而是根据一套染色的规则来进行自平衡，规则如下：
    * 节点颜色要么是红色，要么是黑色
    * 根节点必须是红色
    * 红色节点的两个子节点必须是黑色
    * 每一个叶子节点到根节点的所有路径上的黑色节点数目相同
    * 新插入的节点为红色

    根据这些规则，在插入后可能会导致当前红黑树不符合规则，因此需要针对不同的情况进行左旋、右旋或者双旋来将当前的红黑树恢复到符合规则的状态

    实际上，当我们将红黑树视作四阶B树的时候，也就是一个节点能存储三个值，那么左右两边的值就是红黑树中红色的节点，而中间的那个值就是红黑树中黑色的节点。

### C++ sort的实现，时间复杂度，空间复杂度
1. 快速排序
    其思路主要还是使用分治法：
    * 选择基准值（Pivot）：从数组中选择一个元素作为基准值，常见的选择方法有取第一个元素、最后一个元素、中间元素或随机元素。
    * 分区（Partition）：重新排列数组，使得所有小于基准值的元素放在基准值的左边，所有大于基准值的元素放在基准值的右边。在这个过程中，基准值本身也会找到其正确的位置。
    * 递归排序：递归地对基准值左边和右边的子数组进行快速排序，直到子数组的大小为1或0，此时数组已经完全排序。
2. 时间复杂度
    O(n log n)
3. 空间复杂度
    O(log n)，这个空间复杂度是由递归所决定的，递归的深度大约为log n，因此空间复杂度也是logn

当数据原本就接近有序时，std::sort并不会完全使用快速排序，在这个过程中，如果快速排序的递归深度超过了一定的阈值，这个阈值通常是O(logn)的常数倍，会切换到推排序以保证最坏时间复杂度为O(nlogn)。如果是分区后的子数组大小已经小于某一个阈值了，则会自动切换到插入排序，因为插入排序在小数组上的表现更好。

### 洗牌算法
1. 最简单粗暴的洗牌方法，每一轮都生成一对随机数，交换随机数位置的牌
    ```cpp
    std::random_device rd;  // 种子
    std::mt19937 gen(rd()); // 生成引擎
    std::uniform_int_distribution<> distrib(1, suit.size());
    for(int i = 0; i < suit.size(); i++) {
        int random1 = distrib(gen);
        int random2 = distrib(gen);
        std::swap(suit[random1], suit[random2]);
    }
    ```
    这个算法虽然能在某种程度上实现洗牌，但是效果不够好，没有很好的照顾到suit中的每一张牌，同时，如果两次生成的random1和random2相同，也会导致洗牌次数的浪费。

2. Fisher–Yates Shuffle算法
    ```cpp
    std::random_device rd;  // 种子
    std::mt19937 gen(rd()); // 生成引擎
    vector<int> res;
    for(int i = suit.size() - 1; i > 0; i--)
    {
        std::uniform_int_distribution<> distrib(1, i);
        int random1 = distrib(gen);     // 生成[1, i]的随机数
        res.push_back(suit[random1]);   // 向结果集中插入新数字
        suit.erase(suit.begin() + random1);
    }
    ```
    这个算法在完成一次洗牌动作之后就会缩小下一次的洗牌范围，充分的照顾到了每一张牌，并且解决了暴力法中两次random1和random2相同的可能性，使最终洗牌得到的结构一直保持在$n!$


3. Knuth-Durstenfeld Shuffle算法
    ```cpp
    std::random_device rd;  // 种子
    std::mt19937 gen(rd()); // 生成引擎
    for(int i = suit.size() - 1; i > 0; i--)
    {
        std::uniform_int_distribution<> distrib(1, i);
        int random1 = distrib(gen);     // 生成[1, i]的随机数
        std::swap(suit[random1], suit[i]);  // 交换对应两个位置的牌
    }
    ```
    这个算法是上一个算法的改进，通过原地调换位置的方式，使得空间复杂度从上方的O(n)降低至O(1)

### 寻路算法
1. 深度优先(DFS)
    深度优先搜索是盲目式(意味着不知道终点在哪里的)的寻路算法，并且除非我们把所有可能的路径都走完了，否则DFS给我们的路径通常不是最优解。

2. 广度优先(BFS)
    广度优先搜索同样是盲目式的(意味着不知道终点在哪里的)的寻路算法，但是BFS总是能给我们最优解。

3. 迪杰斯特拉算法
    Dijsktra算法是BFS的改进，在BFS中，所有点的访问顺序都是认为规定好的，比如一个固定的顺序上右下左，但是在Dijsktra算法中，每一次选择下一步都是选择当前记录的累计成本最小的节点。因此在Dijsktra算法中我们需要每一次都遍历一次cost数组找到当前开销最小的点，或者说维护一个优先队列。
    同样的，Dijsktra算法也是盲目式(意味着不知道终点在哪里的)的算法。

4. A* 算法
    A* 算法是一种广泛用于路径规划，游戏AI，地图导航领域的一个寻路算法，相比起迪杰斯特拉算法、BFS、DFS这些寻路算法而言，A* 算法是**启发式**，也就是每一步都会评估到达终点的开销，同时也是一个**最优算法**，也就是得到的路径是从起点到终点的最短路径。
    A*算法的主要工作原理是在每一次选择下一步的过程中，都是从预计成本最低的节点开始查找，这个成本是如何计算的呢？一般来说这个计算成本的函数叫做启发式函数，不同的启发式函数可能会导致不同的搜索效率，常见的有：
    * 曼哈顿距离：在计算一个点到终点的开销使用曼哈顿距离，曼哈顿距离指的是将当前点的坐标和终点坐标的每一个轴的差值绝对值相加，得到的就是曼哈顿距离。这种启发式函数的优点是计算简单，只有常量次数的加减法；而缺点就是曼哈顿距离是不考虑障碍物的，因此可能会导致搜索过程中访问的节点数量变多。
    * 迷宫距离：迷宫距离是一种**会考虑障碍物的启发式函数**，通常是先试用迪杰斯特拉算法进行整一个网格的预计算，然后在启发式函数中使用这个与计算的结果考虑从当前点到终点的最短路径。当然这种启发式函数无疑是优缺点的，就是需要先进行一次迪杰斯特拉算法对整一个地图进行一次最短路径搜索，因此这种算法对于使用同一个起始点然后到达不同位置的这种情况下效果会更好，也就是总共搜索过的网格数量回更少。

### topK问题
1. 优先队列
    使用优先队列维护当前最大的K个数

2. MapReduce
    例如说存在10个1g的文件，统计出这是个文件里面最高频的10个词，内存一次最多只能容纳下一个文件。
    解决方法：通过每一次读取一个文件，统计当前文件中最高频的十个词，这样在完成10次后，我们需要的就是在剩下的100个词中进行去重、取得频率Top10的单词。

当数据量特别大的时候，需要主要读取文件要用流式读取，避免文件比内存当无法读进。比如C++ std::fstream

## Unreal Engine 相关

### UE反射系统
TODO

### UE垃圾回收
UE的垃圾回收主要是用于自动管理内存，其管理的对象是继承自UObject的对象，传统的垃圾回收一般有两种方法，一种是*标记清扫*，另一种是*实时维护引用计数*，UE采用的就是前者，其具有能避免存在循环引用而导致无法释放的优点，但是每一次进行垃圾回收都需要暂停其他的任务。
#### 垃圾回收的简单过程
1. 标记阶段
    UE会遍历所有的UObject对象，标记那些仍然被其他UObject引用的对象，这个过程是从根对象开始，这些根对象会被认为是始终存活的，比如说正在使用的游戏对象和资产，然后递归的调用所有从根能达到的对象。**通过这个标记机制我们可以得知，如果我们想让一个对象不要被垃圾回收，我们可以把对象标记为根对象。**
2. 清除阶段
    UE遍历所有UObject对象，对于上一轮标记阶段中没有被标记上的非根对象，UE会对他们进行内存释放。
3. 压缩阶段(可选)
    在某些情况在，UE可能会进行内存整理，减少内存碎片所导致的内存浪费。

#### UE垃圾回收的具体清洁步骤
1. 首先，引擎会借助`GGarbageCollectionGuardCritical`类中的GCLock函数，在整个GC过程中锁定必要的资源防止其他线程对UObject的操作。
2. 随后就是标记阶段。首先将所有的UObject都标记为不可达，然后对所有根对象进行递归查询，找到所有从根对象能到达的所有资源，并标记为可达
3. 对于上一轮没有标记到的UObject，则表明是不可达的，是需要被释放的。
4. 完成释放后调用`GGarbageCollectionGuardCritical`类中的GCUnlock函数进行解锁，至此垃圾回收完成。

#### 垃圾回收的触发
UE的垃圾回收是自动触发的，基于一定的条件：比如内存使用到到阈值、或者距离上一次进行垃圾回收的时间达到阈值。当然，垃圾回收也是可以通过程序员手动触发的，但一般不会这么做，有可能会影响性能。

#### UE中采用了什么技巧来优化垃圾回收呢？
由于标记追踪的形式会导致整个应用的停止运行，因此需要额外的手段对GC进行加速，
1. 增量回收
    UE的垃圾回收可以分批次进行，而不是一次性处理所有的垃圾对象。这样可以将垃圾回收的开销分散到多个帧中，减轻单帧的GC压力。
    * 分步执行：在增量回收的模式下，整个垃圾回收的过程会被分解成多个小步骤。一帧中可能不会完全执行完一整个垃圾回收的过程，可能会将后续步骤分摊到后续的帧中。
    * 调度：UE的垃圾回收会根据当前的游戏性能和内存使用情况来动态调整增量回收的执行。比如说，当前帧拥有更多的空闲时间，则可以执行更多的回收步骤。就比如说在GC的标记过程中，就可以设置为增量标记的形式，一帧可能不会完整的将所有的对象进行标记，而是将整个标记过程分摊到多帧中。
2. 使用对象池减少回收的开销
    UE采用了对象池来管理重要常见的对象，如特定类型的Actor和组件等，通过重用而不是频繁的销毁和创建，减少了GC释放资源的压力。
3. 并行化
    * 标记阶段：首先在标记阶段，UE就采用了并行标记的方式进行，一个线程会负责一部分的标记任务。
    * 回收阶段：在回收阶段，实际上这个阶段不一定是要在前台进行导致程序停止的，而是对使用一些后台线程完成垃圾回收的某些任务，而主线程则继续处理游戏逻辑和渲染。

### UE渲染流程

### FString和FName的区别
FString是一种动态大小的字符串类型，用于存储和操作可变长的文本数据。类似于CPP标准库中的std::string。**对于需要频繁修改的字符串使用FString更好。**

FName用于存储字符串的唯一实例，通常是用来表示标识符、枚举、资产名称等信息。在FName的内部存在一个全局表用来确保每一个字符串的值只会保存一次。**对于需要频繁比较但不经常修改的场景而言，FName更适合，因为其比较的是一个字符串常亮的引用或指针，而不是比较字符串本身，这节省了比较的性能开销**

### AActor、APawn、ACharacter的区别
这三个类之间存在继承关系，ACharacter -> APawn -> AActor
1. AActor
    是游戏中所有游戏对象的基类，只有一个对象是AActor或者是AActor的派生类才能在游戏关卡中使用。它可以用来表示任何对象，包括静态物体、动态物体、触发器等等。
    * AActor提供了基本的**Transform组件以及于关卡中和其他对象交互的能力**
    * **其默认不附带任何的Component**

2. APawn
    APawn是一个可以被玩家或者AI操控的一个AActor，通常用于表示游戏中的角色、车辆等可控制实体。
    * APawn添加了对输入和控制的支持，允许玩家和AI控制它的行为，具体体现在比如说World Setting中的GameMode部分，Default Pawn Class只能选择继承自Pawn的类。
    * **其默认不附带任何的Component**

3. ACharacter
    ACharacter用于表示具有一定物理行为的角色的类，通常是用来表示游戏中的人物或其他生物。
    * **ACharacter通常包含一个UCharacterMovementComponent处理角色的移动以及USkeletalMeshComponent表示角色的骨骼网格，还有UCapsuleComponent作为角色的碰撞，其中UCharacterMovementComponent是UPawnMovementComponent的子类，提供了更多的移动逻辑，是专门为双脚行走的角色所设计的，默认支持奔跑、行走、跳跃、爬行、游泳的动作。**

    如何替换ACharacter默认的UCharacterMovementComponent？
    ```cpp
    // in MyCharacter.h
    #include ...
    class AMyCharacter : public ACharacter
    {
        GENERATED_BODY()

    public:
        AMyCharacter(const FObjectInitializer& ObjectInitializer = FObjectInitializer::Get());

        // ...
    }

    // in MyCharacter.cpp
    #include ...
    #include "MyCharacterMovementComponent.h"

    AMyCharacter::AMyCharacter(const FObjectInitializer& ObjectInitializer) : Super(ObjectInitializer.SetDefaultSubobjectClass(ACharacter::CharacterMovementComponentName))
    {
        // ...
    }
    ```


## 游戏工程相关
### 帧同步和状态同步
1. 帧同步
    在帧同步中，所有玩家都会以相同的服务器帧率进行游戏，每一个游戏帧游戏都会向服务器发送一次玩家的输入，当服务器收集到所有玩家的信息后，会将这些信息广播到所有玩家中，这样一个多人游戏中的所有所有玩家的操作就能在一个玩家的客户端中还原出来。
    帧同步适用于对实时性要求非常高的游戏，比如格斗游戏或者射击游戏，如cs就是使用帧同步的，只要我们打开网络数据就能看到有一个参数叫做tick，这就是服务器的游戏帧
    但是，帧同步对于网络的延迟要求非常高，这使得一旦网络的状况不太好的情况下，可能会导致玩家的状态不同步。

2. 状态同步
    在状态同步中同步的不是每一帧玩家的输入，而是在一个规定的时间间隔后同步整个游戏的状态，或者是重要的游戏变化。这使得状态同步机制对网络延迟的要求比较低，因为可以不需要每一帧都向服务器进行数据传输，而且可以容忍一定程度的延迟。

### 如何优化大量创建对象的开销
使用对象池，并且在销毁时并不直接销毁而是返回到对象池等待下一次使用，此时就能应对短时间内大量对象的创建所造成的性能压力，缺点就是存在暂时没有使用上的对象占用着内存。

### 对象池的实现方式
对象池会在初始化的时候使用malloc分配大量的可用空间，然后在后续需要的时候使用placement new进行原地构造。从而使得在创建对象的过程中的内存分配过程得到减少，从而减少整体的开销。

### 游戏教程标记完成的实现
可以使用一个uint64来表示一个至多64步的一个教程系统完成情况，每一位都可以用0表示该步未完成，1表示该步已完成
1. 将第n位从未完成标记为已完成(0 -> 1)
    ```cpp
    status |= (uint64_t(1) << (n - 1));
    ```
2. 将第n位从已完成标记为未完成(1 -> 0)
    ```cpp
    status &= ~(uint64_t(1) << (n - 1));
    ```
3. 切换第n位的状态(0 -> 1 OR 1 -> 0)
    ```cpp
    status ^= (uint64_t(1) << (n - 1));
    ```
4. 判断第n位是否完成
    ```cpp
    bool bIsCompleted = status & (uint64_t(1) << (n - 1));
    ```

如何统计究竟完成了多少步呢？
1. 算法
    ```cpp
    int popcount(uint64_t n) {
        int data[16] = { 
            0, 1, 1, 2, 
            1, 2, 2, 3, 
            1, 2, 2, 3, 
            2, 3, 3, 4 
        };
        int count = 0;
        for(int i = 0; i < 16; i++) {
            count += data[n & 0xF];
            n >>= 4;
        }
        return count;
    }
    ```
2. 指令
    `__builtin_popcountll()`这个指令是CPU中一个专门用来统计一个数字中二进制位1的个数

### 游戏中背包系统的优化
1. 大背包数据结构的设计
    * 使用一个数组，对于空物品和存在物品都会填充在数组中，数组的长度就是背包的大小。
    * 使用稀疏数组，只保存拥有的物品，而不会保存空物品的部分。

2. 优化物品频繁的创建和销毁
    使用内存池管理物品，这样就能减少物品频繁创建和销毁锁产生的频繁内存分配和回收带来的性能负担。

3. 优化背包大量物品图片的显示
    * 对于一个存在很多物品要显示的UI界面，可以通过提前加载的方式进行优化：比如说，一个背包的UI显示10个物品，我们可以加载当前页面的前面一个页面和后面一个页面以备用户切换页面。

4. 优化用户的物品搜索
    * **使用索引数据结构**，就像是数据库中为了加速查找数据一样我们也能动态的维护一个哈希表，其中键可以是物品的名称、物品的分类和标签等，值是背包数组中的index。
    * **使用多线程**，对于一个很大的背包来说，可以将整个搜索过程分给多个线程来执行，比如说存在一个大小为10000的背包，可以开10个线程，每一个线程负责搜索其中不重叠的1000个物品，然后将最终得到的结果汇总。
    * **缓存**，对于被搜索了的物品来说，可以构建一个类似于LRU的缓存，将每一次搜索的记录缓存下来以备下次使用。


## 图形学相关

### 现代图形API和老的图形API有什么区别
1. 现代图形API提供了更底层的访问控制，从而让开发者能更精细的控制各种GPU资源和状态。
2. 现代图形API通常也会提供更好的在CPU端的并行支持。比如在D3D12里面可以使用多线程同时对多个CommandList进行指令的录制。
3. 现代图形API通常提供了Pipeline State这种用于记录一个管线流程的对象
    为什么要有Pipeline State呢？
    * 性能优化：通过将所有相关的管线状态预先组合在一起，现代图形API可以减少运行时的状态变更开销。
    * 抽象状态：通过对整一个管线的状态进行抽象，从而达到复用的目的。


### Phong光照模型和Blinn-Phong光照模型
#### Phong 光照
Phong光照是由三个基本的光照部分组成的，是一个经验模型
1. 环境光
    这一部分是用来模拟光线从光源经过其他物体的反射照亮当前物体，是一个常亮光，不依赖于光源位置和观察者的方向。
2. 漫反射
    这一部分是用来模拟光线从光源直接照射到物体粗糙表面并且发生均匀反射的一个效果，具体强度取决于片元法线和片元到光源的法线之间的夹角。
3. 镜面反射
    这一部分是用来模拟光线照射到光滑表面并产生亮点的效果，具体强度取决于光线入射向量对于当前片元的反射向量与片元到摄像机向量的夹角。夹角越小，表示强度越大，得到的最终颜色也就越亮。

#### Blinn-Phong 光照
Blinn-Phong光照同样是由三个基本的光照部分组成，是一个经验模型，可以说是Phong光照的一个改进版，在获得更高的计算效率的同时画面效果相当。
前两种光照的计算方法和Phong光照一致，但是在计算镜面反射的时候是使用片元到光源和片元到摄像机的半程向量和法线向量的点积来计算。

### 碰撞检测是怎么做的？
大致可以分为两个步骤：
1. 粗略检测：通过使用八叉树等数据结构快速排除不可能发生碰撞的物体
2. 精确检测：对粗略检测阶段筛选出的可能碰撞的物体对进行精确的碰撞检测
    要精确计算碰撞，常见的算法有：
    * 分离轴定理（SAT）：通过给两个模型找到一个分离轴，代表这两个模型并没有发生碰撞
    * 基于特征的碰撞检测：使用网格形状的特征来计算是否碰撞，比如求两个球体是否碰撞，就可以用球体的解析式来求
    * 基于网格的碰撞检测：对于复杂的网格形状而言，可以通过使用这两个模型的三角形是否相交来判断

### 如何判断一个点在三角形中
1. 叉积法
    假设点为P，三角形的三个点为ABC，计算AB叉乘AP，BC叉乘BP，CA叉乘CP，如果三个值都是同正或者同负的，则表示点在三角形中。
2. 面积法
    通过计算三角形ABC的面积，和三个三角形ABP、BCP，CAP的面积之和比较，如果面积相等，则表示点在三角形里面。
3. 重心坐标法
    通过计算P点的重心坐标的三个未知数的值，只要解出来三个未知数都是大于0且三数之和为1，则代表点在三角形内。

### 如何计算空间中点到线段的距离

### 渲染管线
![](./pipeline.png)
1. Input Assembler
    这一个阶段负责从内存中获取顶点数据，并将其组装成顶点缓冲区的顶点。这一个阶段还可以定义如何将顶点数据组织成图元（如三角形、线段），
    在代码中，这一部分通常使用`ID3D12GraphicsCommandList::IASetVertexBuffers()`和`ID3D12GraphicsCommandList::IASetIndexBuffers()`这两个函数进行设置，对象为`D3D12_VERTEX_BUFFER_VIEW`和`D3D12_INDEX_BUFFER_VIEW`
2. Vertex Shader
    D3D12中唯一一个必须要存在的一个阶段，用于对每一个顶点进行坐标变换或者计算光照。通过顶点着色器阶段的顶点会被传输到后续的管线阶段
3. Hull Shader
    这个阶段用于细分曲面，通过这个Shader，我们可以控制图元的细分程度，并输出细分后的控制点和细分控制信息给后续的曲面细分器
4. Tesselator
    根据Hull Shader中的曲面细分信息进行曲面细分
5. Domain Shader
    对Tesselator细分后的顶点进行处理，计算其最终的位置。这个阶段会将曲面细分生成的顶点转换为实际的顶点。
6. Geometry Shader
    这个阶段可以对图元进行操作，如添加或者删除顶点，也能改变图元的类型。
    具体Geometry Shader能干什么呢？
    * 实例化渲染，比如说可以在CPU端仅仅准备一个草的模型，然后通过Geometry Shader将其拓展为一片草地。
7. Rasterizer
    将前面阶段的图元转换为像素，确定哪一些像素会被图元覆盖，并对这些像素生成片段，也就是我们说的（Fragment）。在这个阶段中，还会经历裁剪和背面剔除等操作。
    **在Rasterizer和Geometry Shader之间存在一个裁切阶段，目的是将摄像机外的部分裁切掉减少需要计算的数据量**
8. Pixel Shader
    这个阶段会对前面光栅化得到的每一个像素片段进行处理，比如计算颜色、光照、纹理映射等。
9. Output Merger
    最终这个阶段负责将像素着色器的输出合并到渲染目标上，会处理深度、模板测试、混合等操作，生成最终的像素颜色。


### PBR和BRDF
PBR是一种有别于传统使用经验模型的渲染技术，旨在通过模拟现实世界的光照和材料属性
PBR的基本原则有：
1. 能量守恒
2. 基于微观表面的反射模型
3. 使用物理单位进行计算

而BRDF指的是一种用于描述光在物体表面反射特性的一组数学模型，其包括传统的经验模型，也包括有基于PBR的物理光照模型，通常由以下部分组成：
1. 镜面反射
2. 漫反射
3. 菲涅尔效应

常见的BRDF模型包括：
1. Lambertian BRDF：用于描述漫反射
2. Phong BRDF：用于描述高光
3. Blinn-Phong BRDF：Phong BRDF的改进
4. Cook-Torrance BRDF：一种基于微表面理论的物理模型，具有能量守恒的特点，能更准确的描述光线

### 纹理采样方法
1. 最近邻采样
    这种方法会选择最近的纹理像素颜色作为采样的结果，其优点是实现简单，性能高，因为没有额外的计算，但缺点是放大后会有明显的锯齿。
2. 双线性过滤
    这种方法会根据采样点的周围四个纹理像素进行线性差值，能得到更平滑的采样结果，但缺点是性能开销比最近临采样高。
3. 三线性过滤
    这种方法是结合两个相邻层级的mipmap中进行的双线性采样结果再一次进行差值，从而得到在不同距离下都看起来是平滑的采样结果。但缺点是性能开销比双线性采样更高
4. 各项异性采样
    这种方法会考虑屏幕的投影方向和形状，比如当摄像机是斜对着一个面的时候，这种情况下，使用各项异性采样会在不同的方向上对多个点进行加权平均。缺点仍然是性能开销大。

### 什么是延迟渲染，为什么延迟渲染能提高性能
延迟渲染是一种有别于传统正向渲染的渲染技术，能够更高效的处理场景的光照和阴影效果，尤其是存在大量光源的情况下
延迟渲染主要被分为两个阶段：
1. 几何阶段（Geometry Pass）
    在几何阶段中，场景的所有集合信息，包括顶点位置，法线，uv坐标等信息都会被渲染到一组成为G Buffer的纹理中。在此阶段中不会考虑光照
2. 光照阶段（Lighting Pass）
    在光照阶段中，会使用G Buffer中所渲染出来的纹理进行光照的计算，每一个光源只影响它本身所照射的区域，因此光照计算会被限制在光源的影响范围内的像素而不是考虑所有的像素，因此即使场景中存在大量的光源，也不会对性能产生太大的影响。
    而对于传统的正向渲染而言，每多一个光源就会给每一个像素都增加一次计算。当光源增加时，计算量也会呈线性增长。

为什么延迟渲染能提高性能：
* 光照计算效率提高：由于光照计算是在屏幕空间进行的，且只针对受光影响的像素，因此相比于正向渲染，延迟渲染可以更高效地处理大量光源。
* 减少重复的几何处理：在几何阶段，场景中的每个物体只被处理一次，无论场景中有多少光源。这减少了因光源数量增加而导致的重复几何处理。
* 更好的材质处理：延迟渲染允许在光照阶段使用更复杂的材质模型，因为此时已经有了完整的几何信息，可以进行更精确的光照计算。

延迟渲染的缺点：
* 显存占用高，渲染完成后的GBuffer需要保存在显存中以提供接下来的阶段使用。
* 难以应用MSAA，虽然也有别的抗锯齿技术，比如TAA和FXAA等后处理抗锯齿技术，但是效果可能没有传统的提高采样率的MSAA来的好。
* 透明物体处理困难：由于渲染透明物体需要对物体的深度进行排序，但是在延迟渲染中的GBuffer很难有足够的信息去判断多个物体的深度信息，因此需要额外的步骤来实现透明物体的渲染。

延迟渲染如何应用MSAA？
1. 可以简单粗暴的将MSAA应用到GBuffer中的每一张数据，但是这很容易导致显存爆炸。也就是MSAA MRT技术

### 什么是MRT
MRT指的是多重渲染目标，允许一个渲染通道同时渲染到多个纹理目标。

### 阴影怎么做？
1. Shadow Maps
    阴影贴图会从光源的视角渲染一张深度场景，然后在主渲染阶段对于每一个像素使用这张阴影贴图进行深度的判断，具体就是用当前片元的locaiton和光源的location计算出当前片元距离光源的长度，然后从阴影贴图中得到这个光源最近的一个深度信息，如果当前的距离大于在阴影贴图中记录的深度，就意味着当前的片元是被其他物体所遮挡的，应该有阴影。
2. 阴影体积
    阴影体积是一种基于几何形状而非纹理来确定片元是否处于阴影当中的一个制作硬阴影的方法，具体的步骤有：
    1. 构建阴影体积，通常是通过根据从光源方向沿着几何体轮廓延伸完成的。
    2. 渲染阴影体积到模板缓冲区，使用模板缓冲区来记录阴影体积内外的像素。
    3. 使用模板缓冲区的数据渲染场景
3. 软阴影
    * 通过多次不同偏移(比如说泊松盘采样)的对同一个光源进行shadow map采样，然后将这些shadow map进行硬阴影的计算，对于有多张shadow map都覆盖到的片元，则颜色更深
    * 使用过滤技术，比如说PCF(Percentage-Closer Filtering)，具体过程如下：
        1. 对于场景中的每一个像素，计算其在光源视角下的纹理坐标。
        2. 使用该纹理坐标在shadow map中查找对应的深度值。
        3. 在该纹理坐标的周围，一般是一个圆或者一个正方形，随机选取多个点。
        4. 对这些随机选取的点在shadow map中查询，判断像素是否在阴影中。
        5. 通过计算这些采样点在阴影中的比率，作为阴影的不透明度。
        6. 使用这个不透明度混合像素的颜色和阴影的颜色，从而计算出最终的阴影效果。

### early-z是什么
Early-Z是图形渲染中的一种优化手段，用于在执行Pixel Shader前先将要执行的片元深度和深度缓冲中的深度进行比较，如果测试失败就提前丢弃这个片元，不执行其对应的Pixel Shader，从而优化性能。

### z-fighting
z-fighting是3D渲染中会出现的一种视觉现象，当两个或者多个对象或者他们的一部分分厂靠近的时候，他们会拥有非常接近的深度值，这就会导致渲染系统没有办法统一的决定究竟是哪一个对象的面固定在上而另一个在下，就会出现闪烁的现象。
解决方法：
1. 增加深度缓冲区的精度
2. 设置多边形偏移，通过设置偏移让其中一个对象始终在上另一个始终在下。
3. 使用多通渲染，通过多次渲染，第一次渲染第一个对象，第二次渲染第二个对象，这样虽然性能较差，但是可以这个问题

### 深度测试是怎么做的
深度测试是用于决定哪些像素是需要被显示到屏幕上、哪些像素是被遮挡的技术。
流程：
1. 初始化深度缓冲区：在渲染开始的时候，将整个深度缓冲区的值都设为最远，比如说1.0，这个取决于深度的范围。
2. 深度值计算：对于每一个要渲染的像素，都会计算其深度值
3. 深度测试：将计算出的深度值和缓冲区所记录的值进行比较，如果当前像素的深度值小于缓冲区中记录的深度只，则更新为当前的深度值，否则保持不变。
4. 重复上述过程直到计算出整个深度缓冲区

### 模板测试是怎么做的
模板测试是3D图形渲染中的一种常用的用于控制像素绘制的技术。和深度测试类似，过程如下：
1. 设置模板函数：指定模板测试用的比较函数，常见的包括大于小于等于，大于等于。
2. 渲染模板到缓冲区：首先可以将一些初始化的值渲染到模板缓冲区中。
3. 执行模板测试：当渲染场景中的像素时，会根据设置的模板函数和参考值对模板缓冲区中的值进行测试。只有通过测试的像素才会被绘制到颜色缓冲区。
4. 跟新模板缓冲区：根据设置的操作，在模板测试通过或者失败的时候更新模板缓冲区。

### alpha测试
在图形学中，alpha测试是一种基于像素透明度，也就是alpha值来决定是否渲染该像素的技术，过程大致如下：
1. 给每一个像素一个alpha值，通常是0到1之间
2. 设置测试条件，比如说可以设置为大于0.5时渲染
3. 对于每一个像素，如果符合条件则渲染，否则将丢弃


## 计算机体系结构相关
### Cache Miss
三种不同的Cache Miss
1. Compulsory Miss
    当一段数据是由于第一次访问而导致的Cache Miss是属于Compulsory Miss，很难避免。
    * 解决方法：
        通过Prefetch，程序员在将要使用这段数据的情况下提前将某一段数据存入到缓存中，从而使得在第一次使用也能在缓存中获取到数据。

2. Capacity Miss
    当一段数据曾经被读取进入缓存中，但由于缓存空间的大小导致其被交换出去，这种情况下发生的Miss就是Capacity Miss。
    * 解决方法：增大缓存的空间

3. Conflict Miss
    当一段数据曾经被读取进入缓存中，但由于哈希冲突，不同的数据映射到了同一块缓存行中，此时旧的数据就会被交换出去，这是这种Miss就称为Conflict Miss。
    * 解决方法：
        1. Miss Cache
        2. Victim Cache

### 其他能提高缓存性能和内存性能的技术
1. Multibanks Cache


2. 早启动和关键词优先(Early restart and critical word first)
    在使用这个技术的CPU中，CPU并不会完全等待一整段内存完成写入缓存的过程，而是在得到需要的数据后立即启动，和缓存存入数据并行执行。且这种情况下从内存传输到缓存的时候是CPU所需要的数据优先传输，然后再传输一些同一个Cache line中的其他数据。
    **这种技术能减少发生Cache Miss时的Miss Panelty**

3. 写缓冲(Write Buffer)
    当缓存由于交换等因素需要将数据写回到低级Cache或者内存中时，不会先将这段要写入的数据直接写到目标缓存或者内存中，而是在这个阶段之间增加一个叫做Write Buffer的一个独立缓存，这个缓存用于充当一个桥梁，其使得内存和缓存之间或者说是缓存和缓存之间的数据交换的顺序变为：
    1. 将交换出的数据写入到Write Buffer中
    2. 缓存发送数据获取请求(fetch request)
    3. 被请求的缓存或者内存将数据发送到请求的缓存中
    4. Write Buffer将之前交换出来的数据写入到目标缓存中。

## 操作系统相关

### 线程和进程的区别
1. 什么是进程
    进程是计算机执行的基本单位
    进程是资源分配的基本单位，包括的资源有：
    * 一段独立的内存地址，包括代码段，数据段和堆栈
    * 一系列的操作系统资源是按照进程为单位分配的，包括文件句柄，网络连接等

    虽然进程能很好的完成我们的任务，但是在如今的多核CPU中，创建进程和切换进程的开销非常大，首先第一个问题就是每一次切换进程都需要操作系统进行一次上下文切换，虽然这对于现代CPU并不会造成什么压力，但是对于内存和IO这种比较慢速的硬件来说却是一个瓶颈；其次创建进程也需要操作系统为其分配一段新的、独立的内存和操作系统资源；同时进程和进程的通信开销非常的大，对于需要执行同一项任务的两个进程之间难以进行通信，进程间的通信主要通过：管道、消息队列、共享内存或者socket套接字等。
    因此，线程就诞生了

2. 什么是线程
    在出现线程后，内核级线程就成为了CPU调度的基本单位了。
    相比于进程，线程仅仅只维护自己的一个栈，用于分配资源，以及寄存器的状态，用于线程切换。其余的诸如代码段，数据段以及操作系统资源如文件句柄等与其他在同一个进程中的其他线程共享。
    此时，如果我们切换线程而非进程时，切换的开销就大大降低了，尤其是对于用户级线程而言，甚至不需要进行上下文切换。
    同时线程的通信主要通过共享内存的方式，因为同属于一个进程的线程是共享同一段虚拟内存的。

3. 进程和线程的区别
    * 从创建和开销方面来讲：
        创建和销毁进程通常会消耗更多的时间，因为需要操作系统为其分配独立的内存和IO等资源。
        创建和销毁线程的开销则是要小很多，因为一个线程需要维护的仅仅只有其CPU状态：如寄存器状态，栈的位置等。
    * 从独立性上面来讲：
        进程的独立性更好，一个进程的崩溃不会影响到另一个进程
        线程的独立性较差，一个线程的崩溃很可能会影响到另一个线程
    * 从通信机制上来讲：
        进程通常需要使用管道、消息队列、共享内存、socket套接字的方式进行通信，并且一定伴随着进程的上下文切换，这意味着需要一次从用户状态切换到内核状态，开销非常大。
        线程的通信只需要通过共享内存即可，因为同属于同一个进程的线程共享了同一段虚拟内存。就算是使用内核级线程，切换的开销也没有进程大，如果我们使用的是用户级线程，开销就更加小了，甚至不需要操作系统进行模式切换。

4. 什么是协程
    相比于线程和进程，协程仅仅只是一种编程概念，用于实现多任务的处理，可以在单个线程内实现多任务的并发执行。协程的特点包括有：
    * 轻量级：协程的创建和切换开销是远小于线程的，因为这对于操作系统内核而言是透明的，也就是说切换协程不需要进行context switch。同时，在同一个线程中存在的协程是共享同一个栈空间的，这意味着切换协程不需要切换站空间并且使得一个线程支持的协程的数量可以很大。
    * 协作式调度：不用于内核级线程和进程的调度，协程的切换是显式的，由协程自身控制，需要协程自身让权。
    * 简化异步编程：在多数支持协程的语言中，比如golang等，允许开发者使用编写类似于同步方式执行的代码来编写异步编程代码，避免了传统异步编程的回调函数，使得代码的可读性更好。

    **用户级线程和协程的区别：**
    * 用户级线程通常不需要一个线程主动让出资源的所有权，而协程采用的就是协作的方式，也就是每一个协程需要主动让权。
    * 从内存开销上看，协程的内存占用更小，因为一个线程中的协程是共享同一个线程中的堆栈资源的，而用户级线程仍然需要维护一个自己的堆空间。

    **协程的原理：**


5. 进程和线程之间的通信
    1. 线程间通信
        * 共享内存：由于在同一个进程中的线程是共享同一段内存空间的，因此线程能直接访问共享内存进行通信，这是最快的通信方式，但是需要一些同步机制（mutex等）来防止竞争条件。
        * 条件变量：
    2. 进程间通信
        * 管道：管道是一种最基本的IPC机制，允许一个进程将数据写入管道，另一个进程从管道中读取数据。管道可分为：`匿名管道`和`命名管道`。**管道的通信通常是单向、以数据流的形式进行传递的，这就导致了管道中的数据是没有明确的数据边界，并且要实现双向通信需要两个管道**。
        * 信号：信号是用于进程间通信的简单机制，用来通知接收进程某一个事件已经发生。
        * 消息队列：通过使用消息队列，不同进程在有消息的使用向消息队列添加一个新消息，然后接收方在消息队列中去的消息，这样就完成了一次进程间的通信。消息队列这种形式是比较好实现并且能有效防止竞争条件的实现方式，虽然效率比起共享内存来得更差。**虽然看起来消息队列和管道的作用很像，但是消息队列中是以消息作为单位进行通信的，有明确的数据边界，切本质上支持双向通信**。
        * 共享内存：不同进程的虚拟内存可以映射到同一块物理内存中，通过访问和修改这一段共享的物理内存从而达到通信的效果。
        * 信号量：信号量是用来通过多个进程访问共享资源的一个工具
        * 套接字：套接字是一种通过网络传输进行的跨进程通信，支持不同主机之间的不同进程的通信。

### 死锁的形成条件
1. 互斥条件：至少有一个资源必须处于非共享模式，即如果是多个线程想要同时访问，他们只能一个个排队进行访问。
2. 持有并等待条件：一个进程必须至少持有一个资源并等待另一个资源，而这个资源又被另一个进程持有
3. 非抢占条件：资源不能被强制从进程中夺取
4. 循环等待条件：必须存在一个进程等待循环，比如进程A等待进程B的资源，同时进程B等待进程A的资源，此时就会形成一个环。

### 如何解决死锁
1. 提前预防：通过打破死锁形成的四个条件之一，就不会再产生死锁了，比如：所有进程或者说线程，只能通过特定的顺序进行获取资源，比如有A、B、C三个资源，设定所有线程必须以B、A、C的顺序进行获取，这时如果所有线程都能按照约定俗成的顺序进行获取，就不会再产生死锁了。
2. 死锁避免：银行家算法
3. 死锁检测与恢复：在死锁已经发生的前提下，通过检测算法进行检测，发现死锁后使用终止或者回滚的方式进行解决。
4. 使用无死锁设计：比如不要通过共享内存的方式，而是通过消息传递的形式进行通信，从而避免死锁

### CPU调度算法
CPU调度算法的评价标准主要有几个：响应时间、吞吐量等
**不公平的调度算法**
1. FCFS（先来先服务）
    非抢占式的调度算法，先来的任务先执行，就像一个队列一样。
2. SJF（短作业优先）
    分为抢占式和非抢占式
    在进行进程切换的时候，操作系统会选择预计工作时间最短的进程进行执行。对于非抢占式而言，这个抉择只会在当前进程已经执行完毕的时候才会进行，而对于抢占式而言，在新进程加入后也会进行这种判断，如果新加入的进程预计工作时间比当前进程的剩余工作时间还短，则新进来的进程会抢占系统资源。
3. RR（时间片轮转）
    存在一个固定的时间片大小，每一个进程只能在一个时间片中进行执行，时间片结束则切换到下一个进程进行执行。
    这种算法的响应时间的最好的。
4. MLFQ（多层级反馈队列）
    在这种schedular中，存在有多个任务队列，每一个队列代表着不同的优先级，并且每一个队列都能使用不同的调度算法，比如RR和SJF混合使用
**公平的调度算法**
1. Lottery Scheduling（彩票调度）
    相对公平的调度算法
2. Stride Scheduling（步长调度）
    绝对公平的调度算法
3. Completely Fair Schedular(CFS)(Linux使用的公平调度算法)
    相对公平的调度算法，每次选择一个新进程的过程的时间复杂度是O(logn)

### 虚拟内存
物理内存有什么问题？
1. 物理内存的空间是有限的
2. 一个进程可以直接通过屋里地址访问到另外一个进程，这样不安全，破坏了进程的孤立性
3. 直接使用物理内存可能导致部分内存碎片没有办法很好的利用上

虚拟内存是每一个进程所拥有的一个逻辑内存，大致的运行流程在硬件的支持下，CPU首先会将一个虚拟地址发送到MMU（内存管理单元）中进行地址的翻译，翻译得到后的地址就是真实的物理地址，这个地址对于程序员来说是透明的，也就是程序员无法控制，全程由硬件自动完成。

一个虚拟地址可以分为两个大部分，第一个部分是虚拟页号，用于指向页表(Page Table)中的某一条PTE(entry)，第二部分是一个业内偏移量，这个值是永远都不会变的。
| 第一部分 | 第二部分 |
| ------- | ------- |
| Page Number | Offset |

当Page Table中表示该页面存在于内存中，则这个是一个Page Hit，否则就是Page Fault，需要去硬盘中取得该部分数据。

在最基础的虚拟内存中，虚拟内存只有一层，也就是只需要使用虚拟地址查一次表即可。

这个最简单的虚拟内存系统有什么问题？
1. 翻译过程太慢，因为这个PTE是存储于内存中的
2. 一个页表的大小太大了，已经远远能超出物理内存的大小，并且页表是一个进程维护一个的。

要解决第一个问题，首先是引入一个新硬件叫做TLB，这个硬件是在MMU中的一个小缓存，类似于CPU当中所使用的Cache，但是这个TLB的用途只有用来存储PTE而不会存储其他的数据。此时，在引入TLB后的地址翻译过程就变为了
1. CPU发送虚拟地址到MMU
2. MMU首先将虚拟地址在TLB中查询，如果查到了，则将翻译后的物理地址发送会MMU，否则TLB会向内存中查询页表，当取得这条虚拟地址对应的物理地址后，将新地址添加到TLB中，然后将物理地址返回给MMU。
3. 随后MMU会根据这条物理地址去内存、缓存或者硬盘中取得需要的数据。
4. 最后会通过内存或者缓存将数据的数据直接返回给CPU

要解决第二个问题，我们就需要有多级页表了。在多级页表中，我们通过将一些不用的低级页表省略不创建，从而达到减少页表大小的目的，需要注意的是，第一页表是永远会整一个保存在内存当中的。目前的主流CPU都能支持七级页表。在使用多级页表时，虚拟地址的虚拟页号会被拆分成更多的部分，如下所示是一个三级页表的虚拟地址。
| 第一部分 | 第二部分 | 第三部分 | 第四部分 |
| ------- | ------- | --------- | ------- |
| Level 1 Page Number | Level 2 Page Number | Level 3 Page Number | Offset |

## 设计模式

### 单例模式
```cpp
class ThreadSafeSinglton {
private:
	inline static std::once_flag init_flag;

	inline static ThreadSafeSinglton* instance = nullptr;

	static void InitInstance() {
		instance = new ThreadSafeSinglton();
	}

public:
	
	static ThreadSafeSinglton* GetInstance() {
		std::call_once(init_flag, &ThreadSafeSinglton::InitInstance);
		return instance;
	}

};
```

### 工厂模式

### 建造者模式

### 观察者模式
观察者模式是一种一对多的依赖关系，一个主体对象可以被多个观察者所监听，当被观察者发生一定的变化后，将会把这个变化通知给所有的观察者。在游戏中的应用，可以用于进行玩家得分、生命值变化等通知事件。当然，在UE中实际上是以委托的形式进行的。

### MVC模式
MVC模式是软件设计中的一种常见模式，在这种模式下，将应用划分为了三个模块：分别是Model(模型)、View(视图)、Controller(控制器)，在这种设计模式下，实现了用户界面和业务逻辑的分离，从而使得整体架构易于维护和拓展。
* Model
    Model代表程序的数据和业务逻辑。负责与后端进行交互、与数据库交互获取数据。
* View
    View部分主要负责将Model中的数据进行展示。
* Controller
    Controller部分主要负责**接受用户的输入、处理用户的请求和用户与视图之间的交互**。

## 计算机网络

### 7层模型
从底到顶总共7层模型分别是：
1. 物理层
    这一层用于处理电器或者物理接口相关的细节，包括传输数据的媒介
    设备包括：电路、光纤、集线器等
2. 数据链路层
    这一层负责在相邻节点之间建立、维护与终止等功能。
    设备包括：交换机、网桥等
    协议包括：Ethernet(以太网协议)、PPP(点对点协议)
3. 网络层
    这一层用于处理分组在网络中的活动，比如如何选择下一个路由等。
    设备包括：路由器
    协议包括：IP协议
4. 传输层
    这一层负责提供端到端的数据传输，在OSI的模型中，传输层负责保证数据的完整性和可靠性。
    协议包括：TCP(传输控制协议)、UDP(用户数据协议)
5. 会话层
    这一层负责管理会话的建立、维护和终止等功能。
6. 表示层
    这一层确保从一个节点发送到另一个节点的数据能被应用层读取和理解，可以涉及到数据加密、数据格式化等功能。
7. 应用层
    这一层负责为应用软件提供网络服务。
    协议包括HTTP、FTP、SMTP

### 5层模型
1. 物理层
2. 数据链路层
3. 网络层
4. 传输层
5. 应用层：通常包含了7层模型中的会话层、表示层和应用层这三个

### TCP的三次握手和四次挥手
首先来看看TCP报文的头部信息
![](https://pic2.zhimg.com/v2-6aabceb72ad215bf3abf465cbf31a16d_r.jpg)
其中的序号在下方会被表示为seq，而确认号会被表示为ack

1. 三次握手
    1. 发送端往接收端发送一个报文，这个报文中的**SYN = 1，ACK = 0，seq = x**，表示这是一个请求连接报文。
    2. 若是接收端同意同意连接，就会返回一个报文，这个报文中的**SYN = 1，ACK = 1，ack = x + 1，seq = y**，表示这是一个接收连接报文。
    3. 此时连接的发起方收到接受连接报文，就会再次返回一个报文，在这个报文中**SYN = 1，ACK = 1，ack = y + 1，seq = x + 1**。

    *  **为什么不采用两次握手？**
        假如A发送了一次请求连接报文，但是这个报文因为堵塞迟迟没有到B，因此A认为这个连接已经丢失了，就再发出了一个请求，此时第一个报文的堵塞问题消失了并且正确的送到了B中，此时B会认为A请求了两次连接，如果我们没有第三次从请求放发送到被请求方的报文，A和B之间就会建立起两个连接。
    * **为什么不需要四次握手？**
        三次就可以确认连接已经正确确立。使用四次会浪费资源。

2. 四次挥手
    1. 客户端发送报文，这个报文的FIN = 1，意味着这是一个终止连接报文
    2. 此时服务器收到这个终止连接报文，返回一个ACK = 1的报文表示确认终止
    3. 等到服务器处理完成数据后，向客户端发送一个FIN = 1的报文，这同样是一个终止连接的报文
    4. 此时客户端收到服务器的终止连接报文，向服务器发送一个ACK = 1的报文表示确认终止。

    * **为什么需要四次？**
        这是由于TCP连接具有半关闭的特性。TCP在一方结束连接后只是不能再继续发送数据，**但仍保留了接收对方数据的能力**所以两次挥手能关闭整个连接的一半，彻底关闭双方的连接需要4次。
        举个例子：就好像是两个人说话，A对B说，我已经没有话要说了，但是A仍然能接收B所说的话，只是A不会再说话了而已，只有当B也说“我已经没有话要说了”时，这个对话才算是完整结束。

### TCP和UDP的区别
1. 从连接角度来讲
    TCP是面相连接的协议，在传输数据之前需要客户端和服务器先建立连接
    UDP是一种无连接协议，数据可以在没有预先的进行连接的情况在发送数据。
2. 从可靠性来讲
    TCP由于是需要首先建立连接再发送数据的，因此TCP能保证数据能以正确的顺序被接收方所接收，并且提供了一些纠错的机制帮助接收方能辨别这个数据包是否正确。
    UDP由于是不需要建立连接而直接发送数据的，因此发送的数据可能会乱序到达，且没有机制能保证数据在网络传输的过程中没有发生错误等。
3. 从速度和效率方面讲
    由于TCP需要首先建立连接、确认和重传，因此效率比较低。
    而UDP不需要保证数据的正确性，且不需要建立连接，因此效率比较高。
4. 从数据流的类型上来讲
    TCP所传输的数据是字节流
    而UDP所传输的数据是消息，每一个发送的UDP数据报都是独立的消息。
5. 从两种不同协议的头部消息大小来说
    TCP需要20个字节，因为需要包含更多的控制信息，
    UDP只需要8个字节，因为这些控制信息比较少。
6. 从流量控制和拥塞控制方面来说
    TCP提供流量控制和拥塞控制机制
    UDP则不提供

### TCP和UDP共用端口
对于两个不同的协议来讲，他们是可以共用同一个网络端口的，操作系统会根据端口中收到的数据包的协议将数据准确的发送到对应的程序中。

### TCP的拥塞控制
TCP的拥塞控制是一种机制，用于避免网络中的过度拥塞，保证网络的稳定性和效率，这个机制的实现主要包括有四个算法，分别是：
1. 慢启动
    在初始拥塞窗口设置比较小的值，每收到一个ACK，窗口大小翻倍，直到达到慢启动的阈值。
2. 拥塞避免
    当窗口大小达到阈值之后，进入拥塞避免状态，此时，每收到一个ACK，窗口大小只增加当前窗口大小的`1 / cwnd`，也就是线性增长。当发生丢包的时候，将拥塞避免的阈值设置为当前窗口大小的一半，窗口大小重置为1，重新进入慢启动的阶段。
3. 快重传
    快重传的目的是快速恢复丢失的数据包
    当收到三个重复的ACK时，表明在这个ACK之后的数据包丢失了，就需要立即重传丢失的数据包，不用等待重传倒计时。
4. 快恢复
    在快重传后，将 ssthresh 设置为当前 cwnd 的一半，cwnd 设置为 ssthresh 加上 3 个 MSS（因为收到了三个重复的 ACK）。随后，每收到一个重复的 ACK，cwnd 增加 1 个 MSS。当收到新的 ACK 时，表明网络恢复正常，将 cwnd 设置为 ssthresh，重新进入拥塞避免阶段。

### UDP如何实现可靠
1. 添加确认机制
    如同TCP一样，使用UDP的时候可以在应用层中添加确认的机制，在发送方发送了数据包之后会进入等待接收方发回的确认信息。如果在约定的时间内没有收到确认，则重传数据包，也就是**超时重传机制**
2. 添加序列号
    为每一个数据包都添加一个独一无二的序列号，接收方就可以通过这个序列号来检测和丢弃重复的数据包，并且当数据包很大需要分片传输的时候，就可以使用这个序列号在接收方中重新拼凑起来，并且能确保拼凑的顺序正确。

### RPC
RPC是远程过程调用，是一种规范而非一种协议，允许一个程序远程调用另一个程序的过程或者函数，整个调用过程就像是在调用本地函数一样。

在调用的过程中，调用方的RPC库会将调用信息打包，然后通过网络请求将调用请求发送给接收方；当接收方收到请求后，会对这个请求进行解析，获得对应的需要调用的函数信息和参数列表；当执行完成后，接收方同样会将结果打包，变成一个响应信息，并且将这个信息发送会调用方；此时调用方收到响应，将响应信息解包从而得到最终的结果。
PRC不像是HTTP那种网络协议一样，而是一种规范，只要能做到RPC的要求，也就是调用远程过程就像是调用本地过程一样即可，内部的实现可以使用HTTP请求完成，也可以通过其他的方式，只要能完成要求，就是规范的RPC。

RPC的实现有很多种，比较知名的有Google开发的grpc，还有在Java后端中经常用到Dubbo。

### http协议
HTTP协议是一种基于TCP传输协议，用于传输超文本(如HTML)，是一个专门用来在两点之间传输数据的应用层双向协议，一般在HTTP请求头包含内容的长度，内容的数据格式等信息，请求体包含携带的数据。

### http Get请求和Post请求的区别
1. 用途
    Get请求通常是用于在服务器中请求某一个资源，因此对于同一个Get请求来说，每一次请求得到的东西应该是一样的，这就是Get请求的幂等性。
    Post请求可以是每一次进行同一样的请求得到不一样的返回。通常是用来提交表单或者文件上传之类的请求，Post请求可能会修改服务器上的某一些状态。
2. 数据传输形式
    Get请求的参数是直接写在URL中的，因此这些参数对于用户来说是可见的，且参数的数量收到URL的长度限制
    Post请求则是将参数放到请求体中，因此不受到URL长度的限制，且对用户而言是不可见的。
3. 缓存
    Get请求可以被浏览器所缓存
    Post请求通常不会被缓存


## Linux 操作系统 TODO
1. Linux查看进程

2. Linux检测网络

3. grep的用法

## 项目相关
### 能说说你在生存游戏这个项目上遇到的困难吗？
TODO

### 建筑物对齐是怎么做的？如何让建筑物和地面进行法线对齐的？
首先对于存在对齐需求的建筑物，比如说地板，墙壁，天花板这种，先为他们定义不同的碰撞类型，然后，在这些需要对齐的建筑物中添加一些用于碰撞检测的BoxComponent，每一个BoxComponent都会专门监听特定的一种碰撞类型，比如说我可以在地板这个建筑物的四周，加上四个BoxComponent，并且监听的是来自于地板类型碰撞的射线检测，如果碰到了对应类型的射线检测，则把目标位置的世界坐标返回给建造预览的模型，此时这个模型就会显示绿色且对齐到紧挨着的地板上。
对于如何让建筑物和地面进行法线对齐，也是可以通过射线检测的结果来获得，首先，射线检测如果碰撞到东西，返回的信息就包括了碰撞到的面的法线方向，此时由于我们的建筑物是默认朝上的，也就是说我们需要计算的是如何从法线(0,0,1)变成对应的法线方向，实际上UE提供了相应的方法能计算两个向量之间的四元数，在FQuat包下。

### 除了使用碰撞检测外还有什么办法实现建筑物的对齐？
可以将整个地图进行网格化，让所有建筑物的基本单位都成为多少个网格，在尝试放置的时候计算对应位置应该对齐的网格，这样就能保证玩家可以比较轻松的对齐两个地板。但实际上在选定建筑位置的时候就需要使用到射线碰撞检测了，所以我认为使用原本的方法更好用，写起来也更加简洁有效。

### 在实习期间的UI界面主要是怎么做的？有没有用什么框架？
TODO

### 实习期间的项目中插件是怎么做的？
TODO

### 实习期间的智能语音客服系统是怎么做的？困难在哪里？
TODO

### 在Protobuf高效解析的这个项目中，思路大概是什么样的？
1. 首先是在单线程的情况下怎么进一步提高效率
2. 其次是针对于大消息来说，单线程可能存在性能瓶颈，因此考虑使用多线程的方式进行。


## Protobuf相关
### Protobuf的编码形式？为什么Protobuf比起XML和JSON等数据格式拥有更小的体积？
1. VARINT编码形式
    **为什么使用VARINT：**
    Protobuf采用了一种叫做VARINT的编码技术，通过这种技术能将一些大小比较小的数，例如一个int32的数据是60，这种情况下如果我们直接将这个数据编码为二进制，需要四个字节，但是我们知道对于这个比较小的数字来说，要存储他的信息仅仅只需要一个字节即可，此时使用VARINT，就能极大的减少这些数据的大小。
    **具体的编码方式：**
    在VARINT中，我们可以把一个字节视作两个部分，第一部分是一个字节的最高位，这一位用于表示当前的字节是否是一个VARINT的最后一个字节，当这一位为1时，表示后续还有，为0时表示当前字节就是当前VARINT的最后一个字节。
    其次，VARINT的编码是使用小段存储的形式，这是因为通过小端存储，其编码和解析的代码都能写的更简洁高效。同时小段存储也是大多数现代CPU对于数据的存储方式，因此处理器也能很自然的将这些数据进行处理。

2. Protobuf buffer的形式
    对于不同的Protobuf数据类型而言，其数据在序列化后的buffer中的表现形式大致上一样，如下表所示：
    | 第一部分 | 第二部分 |
    | ------ | -------- |
    | 字段的FieldID和TypeID | 数据部分 |

    当然，在后面的数据部分，对于不同的数据类型来说他的存储形式也是有一定去区别的

3. FieldID 和 TypeID
    FieldID 和 TypeID是用来记录后续数据具体类型和对应的字段信息的，用于在解码过程中使用。
    首先，对FieldID进行VARINT编码，然后将得到的结果左移三位，将TypeID编码进整个ID的最低三位。
    为什么只使用最低三位就可以呢？原因是Protobuf的数据大类型可以简单的归为6类，分别对应着ID 0-5，也就是最少能使用三位二进制位能记录下来。

4. 六大类型的编码方式
    | Type ID | 类型 | 用于 |
    | ------- | ---- | ---- |
    | 0 | VARINT | int32, int64, uint32, uint64, sint32, sint64, bool, enum |
    | 1 | I64 | fixed64, sfixed64, double |
    | 2 | LEN | string, bytes, embedded messages, packed repeated fields |
    | 3 | SGROUP | 弃用 |
    | 4 | EGROUP | 弃用 |
    | 5 | I32 | fixed32, sfixed32, float |

    1. 对于ID为0的具体类型中，int32、int64、uint32、uint64、bool、enum，这些类型都是将原本数字的二进制形式转换为VARINT的形式进行序列化。
        而sint32、sint64这两种则是存在区别，他们使用一种叫做zigzag编码的方式，将负数映射为整数的一部分，简单来说，映射的规则如下：
        * 对于正整数而言，其变为原来的两倍，也就是‘1’映射成‘2’
        * 对于负整数而言，起变为原来相反数的两倍减一，也就是‘-1’映射成‘1’(1 * 2 - 1)
    
        我们知道，负数在计算机的存储中是以补码的形式进行存储的，因此对于一个很小的负数，其二进制形式中的许多位都是有效信息，因此采用VARINT编码不仅仅没有减少存储的大小，反而对于sint64可能最多会使用到10个字节来编码一个VARINT。
        而通过使用Zigzag编码，能将小的负数映射到正整数中，减少编码为VARINT之后的体积，从而达到体积上的最优。

    2. 对于ID为1和5的两种类型而言，他们并不会使用VARINT进行数据的存储，而是将整个数据的二进制形式编码进最终的buffer中，因此I64固定占8字节，I32固定占4字节。对于float和double而言，这两者是直接将数据本身的二进制形式编码进最终buffer中。

    3. 对于ID为2的LEN类型数据而言，这种数据是不定长的，用于保存一些如字符串、子消息等数据，在这种类型的数据的数据部分，其头部始终是一个VARINT，用于记录这个变长数据的实际长度，后续则紧接着数据本身，可能是子消息（通过递归的方式进行解码），也可能是字符串和字节流数据（通过将std::string的数据直接编码进最终的buffer中），还有可能是一个可以重复的字段，用repeated表示（分为使用打包编码和非打包编码两种形式）。

5. packed repeated fields的压缩方式
    对于数据类型的repeated数据而言，如int32，sint64，float等，采用的是一种叫做打包编码的形式进行的编码（packed encoding）。
    在了解打包编码的形式之前，我们需要知道非打包编码的方式是怎么进行编码的。对于这些数据，在非打包编码的情况下，每一个单独的数据都会在其前面附带他的FieldID，举个例子，有一个可重复的字段，其Field ID是3，要存入1，2，3这三个数，那么就会打包为：
    ```cpp
    // 实际存储这些数据是以VARINT来进行保存的
    3:1
    3:2
    3:3
    ```
    那么在使用打包编码的情况下，这些数据又是怎么保存的呢？实际上是将所有的数据都转换为VARINT，然后一个接一个的紧密排布，去掉非打包编码前面的FieldID，从而使得最终的buffer大小更小，、且减少了解析式的负担。还是上面那个例子，编码后如下：
    ```cpp
    // 一个接着一个排布
    3:123 //不是一百二十三，而是一二三
    ```

### 和JSON和XML相比，Protobuf的优势和劣势在什么地方？
1. 优势
    * 效率：Protobuf得益于其优秀的编码策略，其在编码和解码的过程中速度都比JSON和XML更快，因为记录下来的直接就是二进制的数据。
    * 紧凑：Protobuf的编码通常数据量比其记录同等信息的XML和JSON格式数据要小很多。
    * 跨语言：Protobuf本身支持多种语言，因此能实现跨语言的数据传输，实际上gRPC就是使用的protobuf进行的数据传输
    * 结构化：通过使用.proto文件进行消息的定义，对于程序员来说结构更清晰，并且同一份proto文件可以用在前后端，保证了前后端能获取到的数据是一致的。

2. 劣势
    * 可读性：由于数据都被序列化为了二进制的形式，因此protobuf编译后的数据对人类来说是不具有可读性的
    * 复杂性：使用Protobuf需要先编写.proto文件然后用专门的编译器进行编译后生成对应的目标代码才能使用，比起JSON等能直接把一个对象转换为JSON字符串来说使用复杂性上提升了不少

## Git相关
TODO

## 反问
二面：
1. 进入项目组后主要是负责什么工作呢？还是先拼UI吗？
2. 公司对实习生有什么培训呢？
3. 在项目中是不是经常使用Lua进行UI开发呢？

HR面：
4. 如果成功进入实习的话是否有机会转正呢？
5. 这个实习岗是暑期实习吗？